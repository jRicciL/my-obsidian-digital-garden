# Big data fundamentals with PySpark

## Big Data?
- Big data is a term used to refer to the study and applications of data sets that are too complex for traditional data-processing software

### The 3V's of Big Data
- ==Volume==: Size of the data
- ==Variety==: Different sources and formats
- ==Velocity==: Speed of processing data

### Terminology

- **Clustered computing:**
	- Collection of resources of multiple machines
- **Parallel computing**
	- Simultaneous computation
- **Distributed computing**
	-	Collection of nodes that run in parallel
- **Batch processing**
	-	Breaking the job into small pieces and running them on individual machines
- **Real-time processing:**
	- Immediate processing of data

### Big data processing systems
- Haddop/MapReduce:
	- Scalabel and fault tolerant framework written in Java
		- Open Source
		- Batch processing
- Apache Spark:
	- Open source
	- Both batch and real-time data processing

## Apache Spark

> Open Source data processing framework and is the engine underneath databricks

^3bb305

### Features of Apache Spark
- Distributed cluster computing framework
- Efficient in-memory computations for large datasets
- Lightning fast data processing framework

### Apache Spark Components

![[Captura de Pantalla 2022-05-04 a la(s) 22.20.31.png]]

### Spark modes of deployment
- Local mode:
	- Single machine such as your laptop
	- Local model convenient for testing, debugging and demostration
- Cluster mode:
	- Set of pre-defined machines
	- Good for production

## PySpark with Python

### Overview
- Apache Spark is written in Scala
- To support Python with Spark, Apache Spark Community release PySpark
- ==PySpark==:
	- Provides computation power similar to Scala
	- `PySpark` APIs are similar to Pandas and Scikit-learn

### Spark shell
- Interactive environment for running Spark jobs
- Helpful for fast interactive prototyping
- Allows interacting with data on disk or in memory

#### PySpark shell
- Supports connecting to a cluster

## SparkContext => `SparkContext`
- `SparkContext` is an entry point into the world of Spark
	- *entry point* => A way to connecting to Spark cluster
	- ðŸ”µ Connects the cluster with the application
	- PySpark automatically creates aÂ `SparkContext`Â for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variableÂ `sc`
	- PySpark automatically creates the SparkContext object namedÂ `sc`

```python
from pyspark import SparkContext
```

- Components:
	- `sc.version` => SparkContext version
	- `sc.pythonVer` => Python version
	- `sc.master` => URL of the cluster of "local" string to run

### Spark Program

```python
from pyspark import SparkContext, SparkConf

configure = SparkConf()\
				.setAppName("name")\
				.setMaster("ip_adress")
sc = SparkContext(conf = configure)
```

### Load data in PySpark

1. SparkContext's `parallelize()` method

```python
rdd = sc.parallelize([1, 2, 3, 4, 5])
```

2. SparkContext `textFile()` method

```python
rdd2 = sc.textFile("text.txt")
```

## Functional Programming
### Functional programming in python 
#### Anonymous functions => LAMBDA
- Functions that are note bound to a name at runtime => `Lambda` functions
	- Syntax -> `lambda arguments: expression`
		- Can have many arguments
		- But only one expression
- Lambda functions are very efficient:
- Combined with:
	- ==MAP== => `map()`
		- `map(function, list)`
	- ==FILTER== => `filter()`
		- `filter()`: returns a new list for which the function evaluates as true
```python
items = [1, 2, 3, 4]
list(filter(lambda x: (x%2 != 0), *items*))
```

# Programming in PySpark RDDâ€™
## PySpark RDD

#### RDD => Resilient Distributed Datasets
- ==Resilient== => Ability to withstand failures
- ==Distributed== => Spanning across multiple machines
- ==Datasets== => Collection of partitioned data:
	- Arrays, Tables, Tuples, etc.

![[Captura de Pantalla 2022-05-06 a la(s) 12.07.14.png]]

### How to create RDD
1. Parallelizing an existing collection of objects
	- `numRDD = sc.parallelize(list)`
2. From external datasets (the most common) => `textFile()`:
	- Files in HDFS
	- Objects in Amazon S3 bucket
	- lines in a text file
3. From existing RDDs

### Partitioning in PySpark
- A partition is a ==logical division== of a large distributed data set
	- The number of partitions can be defined using => `minPartitions()`

![[Captura de Pantalla 2022-05-06 a la(s) 12.10.21.png]]

```python
# Check the number of partitions in fileRDD
print("Number of partitions in fileRDD is", fileRDD.getNumPartitions())

# Create a fileRDD_part from file_path with 5 partitions
fileRDD_part = sc.textFile(file_path, minPartitions = 5)

# Check the number of partitions in fileRDD_part
print("Number of partitions in fileRDD_part is", fileRDD_part.getNumPartitions())
```

## RDD operations in PySpark

![[Captura de Pantalla 2022-05-06 a la(s) 12.14.47.png]]

### ðŸŸ¢ Transformations
- Transformations create new RDDs
- Transformations follow ==lazy evaluation==
	- Spark creates a graph will all the evaluations to be performed
	- The graph starts only when an action is performed on RDD
![[Captura de Pantalla 2022-05-06 a la(s) 12.16.28.png]]

- Common transformations:
	- `map()`
	- `filter()`
	- `flatMap()`
	- `union()`
	
#### MAP transformation
```python
RDD = sc.paralelize([1, 2, 3, ..., n])
# Use map to square each number
RDD_map = RDD.map(lambda x: x * x)
```

![[Captura de Pantalla 2022-05-06 a la(s) 12.27.13.png]]

#### FILTER tranformation
- Filter transformation returns a new RDD with only the elements that pass the condition

![[Captura de Pantalla 2022-05-06 a la(s) 12.28.54.png]]

```python
RDD = sc.paralelize([1, 2, 3, ..., n])
# Use map to square each number
RDD_map = RDD.filter(lambda x: x > 2)
```

#### flatMAP
- `flatMAP()` returns multiple values for each element in the original RDD

![[Captura de Pantalla 2022-05-06 a la(s) 12.30.31.png]]

```python
RDD = sc.paralelize(['hello world', 'how are el braulio'])
# Use map to square each number
RDD_map = RDD.filter(lambda x: x.split(' '))
```

```python
# Create a baseRDD from the file path
baseRDD = sc.textFile(file_path)

# Split the lines of baseRDD into words
splitRDD = baseRDD.flatMap(lambda x: x.split())

# Count the total number of words
print("Total number of words in splitRDD:", splitRDD.count())
```

#### Union Transformation
- Returns the union of one RDD with other RDD

![[Captura de Pantalla 2022-05-06 a la(s) 12.32.04.png]]

```python
Final_RDD = RDD_1.union(RDD_2)
```

### ðŸ”´ Actions
- Actions perform computation on RDDs
- Return a value after running a computation on the RDD
- Basic RDD Actions:
	- `collect()` => Returns all the elements as an array
	- `take(N)` => Returns an array with the first `N` elements from the RDD
	- `first()` => Returns only the first element
	- `count()` => Count the number of elements in the RDD

#### Examples
```python
# Create map() transformation to cube numbers
cubedRDD = numbRDD.map(lambda x: x ** 3)

# Collect the results
numbers_all = cubedRDD.collect()

# Print the numbers from numbers_all
for numb in numbers_all:
	print(numb)
```

```python
# Filter the fileRDD to select lines with Spark keyword
fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)

# How many lines are there in fileRDD?
print("The total number of lines with the keyword Spark is", fileRDD_filter.count())

# Print the first four lines of fileRDD
for line in fileRDD_filter.take(4): 
  print(line)
```

### Working with pair RDDs

- Real life datasets are usually key/value pairs
- in ==Pair RDDs== the key refers to the identifier and the value refers to the data

#### Creating pair RDDs
1. From a list of key-value tuples

3. From regular RDD


#### Common Transformations
- All regular transformations work on pair RDD
- Have to pass functions that operate on key/value pairs

#### Other paired RDD Transformations:
- `reduceByKey()` => Combines values with the same key
	- Runs several parallel operations, one for each key
	- <mark style='background-color: #FFA793 !important'>It is NOT implemented as an ACTION</mark> but as a Transformation

![[Captura de Pantalla 2022-05-06 a la(s) 12.47.10.png]]

```python
# Convert the words in lower case and remove stop words from the stop_words curated list
splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)

# Create a tuple of the word and 1 
splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))

# Count of the number of occurences of each word
resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x, y: x + y)
```

- `groupByKey()` => Group values with the same key

![[Captura de Pantalla 2022-05-06 a la(s) 12.51.28.png]]

- `sortByKey()` => Return an RDD sorted by the key
	- It returns an RDD sorted by key in ascending or descending order
![[Captura de Pantalla 2022-05-06 a la(s) 12.50.40.png]]

```python
# Sort the reduced RDD with the key by descending order
Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)

# Iterate over the result and retrieve all the elements of the RDD
for num in Rdd_Reduced_Sort.collect():
  print("Key {} has {} Counts".format(num[0], num[1]))
```

```python
# Display the first 10 words and their frequencies from the input RDD
for word in resultRDD.take(10):
	print(word)

# Swap the keys and values from the input RDD
resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))

# Sort the keys in descending order
resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)

# Show the top 10 most frequent words and their frequencies from the sorted RDD
for word in resultRDD_swap_sort.take(10):
	print("{},{}". format(word[1], word[0]))
```

- `join()` => Joins two pair RDDs based on their key

![[Captura de Pantalla 2022-05-06 a la(s) 12.52.20.png]]

### More Actions

### `reduce()`
- Action used for aggregating the elements of a regular RDD
- The function should be commutative and associative

### `saveAsTextFile()`
- Recomended to avoid using `collect()` in very huge RDDs
	- Data is collected/saved into HDFS or AmazonS3 storage systems
- <mark style='background-color: #9CE684 !important'>Each partition</mark> is saved into a separate file inside the same directory

### `countByKey()`
- Only available for pair RDDs
- Count the number of elements for each key
- âš ï¸ Should be used when memory can fit the size of the data... since it is an action

```python
# Count the unique keys
total = Rdd.countByKey()

# What is the type of total?
print("The type of total is", type(total))

# Iterate over the total and print the output
for k, v in total.items(): 
  print("key", k, "has", v, "counts")
```

### `collectAsMap()`
- Returns the key-value pairs in the RDD as a dictionary
- âš ï¸ Used only if data fits in memory

# PySpark DataFrames

- #PySparkSQL is a Spark library for structured data.
	- Provides more information about the structure of data and computation.
	- `DataFrame` => is an immutable distributed collection of data with named columns
		- Similar to #SQL tables
		- Designed for processing both structured and semi-structured data ( #JSON )
		- In #PySpark support both #SQL and expression methods

### SparkSession:
- `SparkSession` => Provides a single point of entry to interact with Spark DataFrames
	- Used to:
		- Create DataFrames
		- Register DataFrames
		- Execute SQL queries
	- Exposed in the shell as the variable `spark`
	
### Create DataFrames
1. From existing RDDs using SparkSession `createDataFrame()` method
2. From various data sources => `csv`, `json`, `txt`

#### Schema
- ==Schema== controls the data and helps DataFrames to optimize queries
	- Provides information about column names, data types, empty values, etc.

![[Captura de Pantalla 2022-05-08 a la(s) 11.35.23.png]]

![[Captura de Pantalla 2022-05-08 a la(s) 11.36.48.png]]

#### Examples

```python
# FROM RDD
# Create an RDD from the list
rdd = sc.parallelize(sample_list)
# Create a PySpark DataFrame
names_df = spark.createDataFrame(rdd, schema=['Name', 'Age'])
# Check the type of names_df
print("The type of names_df is", type(names_df))


# FROM FILE
# Create an DataFrame from file_path
people_df = spark.read.csv(file_path,
header=True, inferSchema=True)
# Check the type of people_df
print("The type of people_df is", type(people_df))
```


## Interacting with PySpark DataFrames
- Operations
	- Transformations:
		- `select()`
		- `filter()`
		- `gorupby()`
		- `orderby()`
		- `dropDuplicates()`
		- `withColumnRenamed()`
	- Actions:
		- `head()`
		- `show()`: prints the first 20 rows
		- `count()`
		- `describe()`
		- `columns` => attribute to return the name of. columns
	- `printSchema()` => Check the type of columns inside the dataframe

## Interacting using SQL queries
- We can use SQL queries
- SQL queries can be concise an easier to understand and portable
- Executed using the `.sql()` method
- It requires to create a temporary SQL table with the following cod:

```python
# Create the temporary table 
df.crateOrReplaceTempView('table_temp')

# Operate on the temo_table and create a new dataframe
df2 = spark.sql('''
SELECT field1, field2 FROM table_temp
''')

df2.collect()
``` 


```python
# Create a temporary view of fifa_df
fifa_df.createOrReplaceTempView('fifa_df_table')

# Construct the "query"
query = '''SELECT Age FROM fifa_df_table WHERE Nationality == "Germany"'''

# Apply the SQL "query"
fifa_df_germany_age = spark.sql(query)

# Generate basic statistics
fifa_df_germany_age.describe().show()
```

## Data Visualization in Pyspark

- There are three ways to create data visualizations using #PySpark:
	1. `pyspark_dist_explore` library
	2. `toPandas()` method
	3. HandySpark library

#### Pyspark_dist_explore
- Library used to provide quick insights into DataFrames
- It has three functions available:
	1. `hist()`
	2. `displot()`
	3. `pandas_histogram()`

#### Using Pandas for plotting <- `toPandas()`
- Consists on convert the `DataFrame` to a pandas dataframe
	- ðŸš¨ Remember that Pandas DataFrames are in-memory => So be careful when converting PySpark DataFrames to Pandas
	- Pandas DataFrames are mutable and PySpark ones are not
	- Pandas DataFrames are not operated in a lazy manner and they are not ran in parallel

```python
# Check the column names of names_df
print("The column names of names_df are", names_df.columns)
# Convert to Pandas DataFrame  
df_pandas = names_df.toPandas()
# Create a horizontal bar plot
df_pandas.plot(kind='barh', x='Name', y='Age', colormap='winter_r')
plt.show()
```

#### #HandySpark method of Visualization
- ==HandySpark== is a package designed to improve PySpark visualizations

***
# PySpark #MLlib

## Overview
- More about ML in #PySpark -> [[C5. ML with Spark]]
- `MLlib` is a component of Apache Spark for machine learning:
	- Collaborative filtering
	- Classification and Regression
	- Clustering
	- Featurization:
		- Feature Extraction
		- Transformation
		- Dimensionality Reduction
		- Selection
	- Pipelines:
		- Construct, Evaluate and Tune ML pipelines
- ðŸŸ¢ MLlib only contains algorithms that can be operated in parallel processing
- `pyspark.mllib` => Only supports ==RDD== objects
- It is good for iterative algorithms

![[Captura de Pantalla 2022-05-08 a la(s) 17.31.17.png]]

```python
# Import the library for ALS
from pyspark.mllib.recommendation import ALS

# Import the library for Logistic Regression
from pyspark.mllib.classification import LogisticRegressionWithLBFGS

# Import the library for Kmeans
from pyspark.mllib.clustering import KMeans
```

## Collaborative Filtering
- Basic understanding to ==Collaborative Filterning== and ==Recommender systems==
	- [[collabora]]
- Collaborative filtering:
	- Finding users that share common interests
	- Commonly used for recommender systems
	- Approaches:
		1. **User-User Collaborative Filtering**:
			- Finds users that are similar to the target user
		2. **Item-Item Collaborative Filtering**:
			- Finds and recommends items that are similar to items with the target user

### Recommendation systems in PySpark

- The `Rating` class is a wrapper around tuple (user, product and rating)
- Useful for parsing the RDD and creating a tuple of user, product, and rating

```python
# Create the Rating object with the following data
from pyspark.mllib.recommendation import Rating
r = Rating(user = 1, product = 2, rating = 5.0)
```

### Splitting the data using `randomSplit()`
- Splitting data intro training and testing sets
- Simple example:

```python
data = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8])
training, test = data.randomSplit([0.6, 0.4])
training.collect()
test.collect()
```

### Alternating Least Squares #ALS
- #ALS use =>

```python
from spark.mllib.recommendation import ALS

ALS.train(ratings, rank, iterations)
```

- `ratings` => An RDD object with `Rating` objects
- `rank` => represents the number of features
- `iterations` => num of iterations to run the least squares computation

![[Captura de Pantalla 2022-05-08 a la(s) 17.45.52.png]]

### Predicting => RDD of Rating Objects: `predictAll()`
- Returns a list of predicted ratings for input user and product pair
	- It takes an RDD without Rantings

```python
predictions = model.predictAll(unrated_RDD)
predcitions.collect()
```

### Model Evaluation using MSE

```python
true_rates = ratings.map(lambda x: (x[0], x[1], x[2]))
true_rates.collect()

preds = predictions.map(lambda x: (x[0], x[1], x[2]))
preds.collect()
```

#### Example

##### Load dataset into RDDs

```python
# Load the data into RDD
data = sc.textFile(file_path)

# Split the RDD 
ratings = data.map(lambda l: l.split(','))

# Transform the ratings RDD 
ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))
```

##### Model training and predictions

```python
# Split the data into training and test
training_data, test_data = ratings_final.randomSplit([0.8, 0.2])

# Create the ALS model on the training data
model = ALS.train(training_data, rank=10, iterations=10)

# Drop the ratings column
testdata_no_rating = test_data.map(lambda p: (p[0], p[1]))

# Predict the model
predictions = model.predictAll(testdata_no_rating)

# Return the first 2 rows of the RDD
predictions.take(2)
```

##### Model evaluation using MSE

```python
# Prepare ratings data
rates = ratings_final.map(lambda r: ((r[0], r[1]), r[2]))

# Prepare predictions data
preds = predictions.map(lambda r: ((r[0], r[1]), r[2]))

# Join the ratings data with predictions data
rates_and_preds = rates.join(preds)

# Calculate and print MSE
MSE = rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean()
print("Mean Squared Error of the model for the test data = {:.2f}".format(MSE))
```

## Classification

### Example with [[Logistic Regression]]

- PySpark `MLlib` contains specific data types `Vectors` and `LabelledPoint`

#### `Vector()`
- Two types of vectors:
	- **Dense Vector**:
		- Stores all the entries in an array of floating point numbers
	- **Sparse Vector**:
		- Store only the nonzero values and their indices

![[Captura de Pantalla 2022-05-08 a la(s) 21.46.53.png]]

#### `LabelledPoint()`
- A `LabelledPoint` is a wrapper for input features and predicted value:
	- Arguments:
		- 1 -> Label
		- 2 -> The Vector

![[Captura de Pantalla 2022-05-08 a la(s) 21.49.21.png]]

### `HashingTF()`
- `HashingTF()` algorithm is used to map feature value to indices in the feature vector

```python
from pyspark.mllib.feature import HashingTF

sentence = 'hello hello world'
words = sentence.split()
tf = HashingTF(10000)
tf.transform(words)
```

- Result

```
SparseVector(10000, {3065: 1.0, 6861: 2.0})
```

### Logistic Regression
- Using `LogisticRegressionWithLBFGS`
- It requires at least an RDD of LabelledPoint

### EXAMPLES

##### Loading data
```python
# Load the datasets into RDDs
spam_rdd = sc.textFile(file_path_spam)
non_spam_rdd = sc.textFile(file_path_non_spam)

# Split the email messages into words
spam_words = spam_rdd.flatMap(lambda email: email.split(' '))
non_spam_words = non_spam_rdd.flatMap(lambda email: email.split(' '))

# Print the first element in the split RDD
print("The first element in spam_words is", spam_words.first())
print("The first element in non_spam_words is", non_spam_words.first())
```

##### Feature hashing and LabelPoint

```python
# Create a HashingTf instance with 200 features
tf = HashingTF(numFeatures=200)

# Map each word to one feature
spam_features = tf.transform(spam_words)
non_spam_features = tf.transform(non_spam_words)

# Label the features: 1 for spam, 0 for non-spam
spam_samples = spam_features.map(
    lambda features: LabeledPoint(1, features))
non_spam_samples = non_spam_features.map(
    lambda features: LabeledPoint(0, features))

# Combine the two datasets
samples = spam_samples.join(non_spam_samples)
```

##### Logistic Regression model training

```python
# Split the data into training and testing
train_samples,test_samples = samples.randomSplit([0.8, 0.2])

# Train the model
model = LogisticRegressionWithLBFGS.train(train_samples)

# Create a prediction label from the test data
predictions = model.predict(test_samples.map(lambda x: x.features))

# Combine original labels with the predicted labels
labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)

# Check the accuracy of the model on the test data
accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count() / float(test_samples.count())
print("Model accuracy : {:.2f}".format(accuracy))
```

## Clustering

- PySpark supports:
	- K-means
	- Gaussian mixture
	- Power iteration clustering (PIC)
	- Bisecting k-means
	- Streaming k-means

### K-Means with PysSpark
![[Captura de Pantalla 2022-05-08 a la(s) 22.07.27.png]]

```python
# Load the dataset into an RDD
clusterRDD = sc.textFile(file_path)

# Split the RDD based on tab
rdd_split = clusterRDD.map(lambda x: x.split('\t'))

# Transform the split RDD by creating a list of integers
rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])

# Count the number of rows in RDD 
print("There are {} rows in the rdd_split_int dataset".format(rdd_split_int.count()))
```

### Evaluating the K-Means model

![[Captura de Pantalla 2022-05-08 a la(s) 22.08.01.png]]

```python
# Train the model with clusters from 13 to 16 and compute WSSSE
for clst in range(13, 17):
    model = KMeans.train(rdd_split_int, clst, seed=1)
    WSSSE = rdd_split_int.map(
        lambda point: error(point)).\
        reduce(lambda x, y: x + y)
    print("The cluster {} has Within Set Sum of Squared Error {}".format(clst, WSSSE))

# Train the model again with the best k
model = KMeans.train(rdd_split_int, k=15, seed=1)

# Get cluster centers
cluster_centers = model.clusterCenters
```

### Visualizing K-Means

![[Captura de Pantalla 2022-05-08 a la(s) 22.09.35.png]]