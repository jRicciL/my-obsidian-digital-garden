# Data Cleaning with Spark
#DataCamp 

https://campus.datacamp.com/courses/cleaning-data-with-pyspark
## DataFrame Details

Process in data cleaning:
1. Reformatting or replacing text
2. Performing calculation
3. Removing garbage or incomplete data

Advantages of Spark:
- Scalable
- Powerful framework for data handling

### Spark Schemas
- Define the format of a DataFrame
	- Used to validate the data 
- May contain various data types:
	- Strings, dates, integers, arrays
- Can filter garbage data during import
- Improves read performance

#### Example Spark Schema
- Each `StructField` includes the name of the field, the data type, and whether it can include `null` entries.

```python
import pyspark.sql.types

peopleSchema = StructType([
	# Define the name field
	StructField('name', StringType(), True),
	# Add the age field
	StructField('age', IntegerType(), True),
	# Add the city field
	StructField('city', StringType(), True)
])
```

- Read `csv` file containing data using the ==Schema==

```python
people_df = spark.read.format('csv')\
			.load(name = 'rawdata.csv',
				  schema = peopleSchema)
```

### Immutability and Lazy Processing

- Spark DataFrames are ==immutable==
	- Immutability is usually present in Functional programming
	- Defined once
	- Unable to be directly modified
	- Able to be shared efficiently
	- Recreated if reassigned

```python
# Load the CSV file
aa_dfw_df = spark.read.format('csv')\
				 .options(Header=True)\
				 .load('AA_DFW_2018.csv.gz')

# Add the airport column using the F.lower() method
aa_dfw_df = aa_dfw_df\
				.withColumn('airport', 
							F.lower(aa_dfw_df['Destination Airport']))

# Drop the Destination Airport column
aa_dfw_df = aa_dfw_df.drop(
				aa_dfw_df['Destination Airport'])

# Show the DataFrame
aa_dfw_df.show()
```

### Understanding #Parquet

Difficulties with CSV files

- No defined schema
- Nested data requires special handling
	- Identated data
- Encoding format limited
- Slow to parse in Spark
- If no `Schema` is defined =>
	- All data must be read before infer the data types
- Files cannot
- Predicate Pushdown:
	- files cannot be filtered
- Any intermediate use requires redefining schema

## The ==Parquet== Format
- A columnar data format developed for using in any Hadoop based system
	- Binary file format
- Supported in Spark and other data processing frameworks
- Supports ==predicate pushdown==:
	- Leverages the structure of parquet to improve query performance.
	- Ability of a query engine to push filters or conditions directly into the storage layer â†’ Before the data is even read by the query engine.
- Automatically **stores schema information**
- Used to perform SQL operations
	- As backing stores for SparkSQL

### Working with Parquet
- Difficulties with CSV files:
    - The schema is not defined
    - Nested data requires special handling
    - Encoding format limited
    - In spark:
        - CSVs are slow to parse
        - Files cannot be filtered
### Parquet files
- Loading `Parquet` files

```python
# First option
df = spark.read.format('parquet').load(parquet_file)

# Second option
df = spark.read.paquet(parquet_file)
```

- Writing `Parquet` files

```python
# First option
df = spark.write.format('parquet').sav(parquet_file)

# Second option
df = spark.write.paquet(parquet_file)
```

- `parquet` is a compressed columnar data format developed for use in any Hadoop based system.
    - Are binary file format and can only be used with the proper tools
- Automatically stores schema information.
- Supports **predicate pushdown**
    - _This means Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets._

```python
df = spark.read.format('parquet').load('filename.parquet')

# Save the df3 DataFrame in Parquet format
df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')
```

- Are perfect to perform sparkSQL operations>
    - Parquet files are perfect as a backing data store for SQL queries in Spark.

```python
flight_df = spark.read.parquet('flights.parquet')
# Create a temp view using the table and assigning an alias
flight_df.createOrReplaceTempView('flights')

# Query the table
short_flights_df = spark.sql(
	'SELECT * FROM flights WHERE flightduration < 100'
)
```


### Parquet and SQL

```python
spark.sql('''
		  SELECT *
		  FROM flights
		  WHERE flightduration < 100
		  ''')
```

#### Exercises

```python
# Read the Parquet file into flights_df
flights_df = spark.read.parquet('AA_DFW_ALL.parquet')

# Register the temp table
flights_df.createOrReplaceTempView('flights')

# Run a SQL query of the average flight duration
avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]
print('The average flight time is: %d' % avg_duration)python

```

## Manipulating DataFrames in the real world

DataFrames:
- Made up of rows and columns
- Immutable
- use various transformation operations

### Common DataFrame transformations
- `Filter`
	- Remove nulls
	- Remove odd entries
	- Split data from combined sources
	- Negate with `~`

![[Captura de Pantalla 2022-05-15 a la(s) 20.18.14.png]]
Filter / Where

```bash
df.filter(df['col_name'] > "1/1/2019")
df.where()
```

- `select`
- `withColumn`

### Filtering data

- Remove nulls
- Remove odd entries
- Split data from combined sources


- `select()`
- `withColumn()`
- `drop()`

```python
# Return rows where name starts with "M"
voter_df.filter(
	voter_df.name.like('M%')
)

# Return name and position only
voters = voter_df.select('name', 'position')
```

### Column string transformations

- use `pyspark.sql.functions`

```python
import pyspark.sql.functions as F

F.upper('column')
F.split('column')
```

![[Captura de Pantalla 2022-05-15 a la(s) 20.19.59.png]]

### `ArrayType()` column functions
- Various utility functions / transformations to interact with `ArrayType()`
- These are ==methods== that operate over columns:
	- `.size(<column>)` -> returns length of `arrayType()` column
	- `getItem(<index>)` -> used to retrieve a specific item at index of list column

#### Examples

```python
# Show the distinct VOTER_NAME entries
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)

# Filter voter_df where the VOTER_NAME is 1-20 characters in length
voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')

# Filter out voter_df where the VOTER_NAME contains an underscore
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))

# Show the distinct VOTER_NAME entries again
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```

```python
# Add a new column called splits separated on whitespace
voter_df = voter_df.withColumn('splits', 
                        F.split(voter_df.VOTER_NAME, '\s+'))

# Create a new column called first_name 
# based on the first item in splits
voter_df = voter_df.withColumn('first_name', 
                        voter_df.splits.getItem(0))

# Get the last entry of the splits list and 
# create a column called last_name
voter_df = voter_df.withColumn('last_name', 
                        voter_df.splits.getItem(
                            F.size('splits') - 1))

# Drop the splits column
voter_df = voter_df.drop('splits')

# Show the voter_df DataFrame
voter_df.show()
```

## Conditional DataFrame column operations

### Conditional clauses

- `F.when()`
	- In line version of `if then else` operations
	- Allows to conditionally modify a Data Frame based on its content

```python
df.select(
	df.Name, df.Age,
	F.when(df.Age > 18, "Adult")\
	 .when(df.Age < 18, "Minor")\
	 .othervise("Has 18")
)
# Returns the following
```

| Name    | Age |       |
| ------- | --- | ----- |
| alice   | 14  | Minor |
| Bob     | 28  | Adult |
| Candice | 38  | Adult |
| Chikis  | 18  | Has 18      |

- Other example
```python
pets_df = pets_df.withColumn(
			"age",
			F.when(pets_df.Name == 'Brownie', F.rand())
			 .when(pets_df.Name == 'Seresto', 5)
			 .otherwise(2)
)
```

## User defined functions 

- #UDF â†’ Python method defined by the user
	- Wrapped via the `pyspark.sql.functions.udf` method
	- Stored as a variable
	- Called like a normal Spark function

### Example â†’ Reverse string UDF

- #UDF with arguments

```python
# user defined function
def reverse_string(my_str):
	return my_str[::-1]

# Wrap the function and store as a variable
udfReverseString = udf(reverse_string, StringType())
# Include the returning object type

# use with sparl
user_df = user_df.withColumn('ReverseName',
							udfReverseString(user_df.Name))
```

### UDF â†’ Argument-less example

```python
def sorting_cap():
	return random.choice(['G', 'H', 'R', 'S'])

udfSortingCap = udf(sorting_cap, StringType())

user_df = user_df.withColumn('Class', udfSortingCap())
```

## Partitioning and Lazy processing

- DataFrames are broken up into partitions
- Partition size can vary â†’ But Sparks tries to keep they equal
- Each partition is handled independently

### Lazy processing
- ==Transformations are lazy==
	- Nothing is actually done until an ==action== is performed
- Transformations can be-reordered for best performance

### Get the number of partitions

```python
df.rdd.getNumPartitions()
```

### Monotonically increasing IDs

- IDs are assigned based on the DataFrame partition 
	- The ID values may be much greater than the actual number of rows in the DataFrame
	- ðŸŸ¡ IDs are not actually generated until an action is performed 

```python
pyspark.sql.functions.monotonically_increasing_id()
```

- Provides an Integer (64-bit), increases in value, unique
	- Not necessarily sequential 
	- completely parallel

```python
# Select the unique razas of mascotas
df = df.select(df["RAZA_MASCOTAS"]).distinct()

# Count the rows in df DataFrame
print(df.count())

# Add an ROW_ID
df = df.withColumn("ROW_ID", F.monotonically_increasing_id())

# Show the rows with 10 highest IDs in the set
df.orderBy(df.ROW_ID.desc()).show(10)
```

### ID Tricks â†’ Get the max ID of a previous process and used it to avoid overlap with a new process

```python
previous_max_ID = df.select('ROW_ID').rdd.max()[0]

# add a row_id column to new DF starting at the desired value
df_new = df_new.withColumn("ROW_ID", 
			  F.monotonically_increasing_id() + previous_max_ID)
```