#Book 
#Spark →Analytics engine for large-scale application


## Spark Architecture

![[Pasted image 20231230201435.png]]
### Overview of a ==Spark Cluster== 

![[Pasted image 20240105072038.png]] ^9c1d86
- A master node → ==Cluster manager==
	- Managing Spark applications
	- Grant resources for the application
- A set of ==Worker== (Executor nodes) 
	- Responsible for executing tasks

## Spark Session

> *The entry point to programming Spark with the RDD and DataFrame API.*

^a325d8

- Starting a ==PySpark== shell gives two global variables/objects:
	- `spark` → An instance of `SparkSession`
		- Ideal for creating DataFrames
	- `sc` → An instance of `SparkContext`
		- Ideal for creating RDDs
- In a self contained Pyspark application (python driver) you need to create an instance of `SparkSession` manually:

### Create a `SparkSession`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder\ # 1
		.master("local")\
		.appName("my-app-name")\
		.config("spark.some.config.option", "some.value")\ # 2
		.getOrCreate()
```
^087066

1. `builder` → provides access to the builder API used to construct SparkSession instances.
2. `config` → Sets configuration options

### SparkSession usability
- Create DataFrames
- Register DataFrames as tables
- Execute SQL over tables and cache tables
- Read/write data sources in different formats
- Read/write relational databases

## Spark Context

> The **main entry point** for Spark functionality. Represents the **connection** to a Spark cluster, and can be used to create RDD and broadcast variables on that cluster.

- Create a reference to the `SparkContext`
```python
sc = spark.SparkContext
```
- If you will be working only with RDDs, create an instance of SparkContext as follows
```python
from pyspark import SparkContext

spark_context = SparkContext("local", "myapp")
```