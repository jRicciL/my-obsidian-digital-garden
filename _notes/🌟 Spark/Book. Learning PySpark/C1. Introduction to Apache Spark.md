# C1. Introduction to Apache Spark
#Book #Spark 

## What is Apache Spark
> Apache #Spark is a **unified engine** designed for **large-scale distributed data processing** on premises in data centers on in the cloud.
- Provides in-memory storage for intermediate computations.
- Incorporates libraries with composable APIs for:
	- Machine Learning
	- SQL for interactive queries
	- Stream processing 
	- Graph processing (GraphX)
### Spark’s design philosophy
- **Speed**:
	- Internal implementation benefits from hardware → Multithreading and parallel processing.
	- Builds its query computations as a directed acyclic graph (DAG)
		- DAG scheduler and query optimizer → Construct an efficient computational graph.
		- Physical execution engine → ==Tugsten== → Whole storage code generation to generate compact code for execution
- **Easy to use**
	- #RDD → Resilient Distributed Dataset
		- A fundamental abstraction of a simple logical data structure
		- Is the base of other high-level data abstractions → #DataFrame and #Dataset 
	- Provide *transformations* and *actions* as operations
- **Modularity**
	- Spark operations applicable across many types of workloads
	- Different programming languages: Scala, Java, Python, SQL and R
	- APIs → Spark SQL, Spark Structured Streaming, Spark #MLlib, and GraphX
- **Extensibility**
	- Spark focuses on parallel computation engine rather than on storage
	- Spark decouples computation from storage.
		- Spark can read data from multiple myriad sources.
![[Pasted image 20231217144220.png]]

## The Genesis of Spark
- Google File Systems, #MapReduce and Bigtable
- ==Map Reduce== → New parallel programming paradigm 
	- Based on functional programming
- Hadoop File system → #HDFS 
	- Donated to the Apache Software Foundation
	- Map Reduce framework on HDFS shortcomings:
		- Hard to manage and administer
		- Batch-processing Map reduce → Verbose API and lot of setup code with little fault tolerance
		- Partial results are written to Disk → Slow computation times due I/O processes
- Spark early years at AMPLab
	- Researchers from UC Berkeley with experience on Hadoop Map Reduce
	- #Spark → Simple, faster, and easier.
	- Early stages → 20 to 30 times faster than Hadoop
	- Improvements:
		- Highly fault tolerance
		- In-memory storage
		- Easy and composable API → Multiple languages
		- Embarrassingly parallel

## Apache Spark components
Unified engine for Big Data processing

Apache Spark components as a **unified stack**:
- The underlying code is decomposed into highly compact bytecode that is executed in the workers’ JVM across the cluster.
- **Spark SQL**
	- Works well with structured data
	- Construct permanent or temporary tables
	- SQL-like queries
- **Spark MLlib**
	- Contains common machine learning algorithms
	- Since Spark 1.6 it was split into two packages:
		- `spark.mlib` → RDD-based API; in maintenance mode.
		- `spark.ml` → DataFrame base API → New features
		- These APIs → extract and transform features
- **Spark Structured Streaming**
	- Combine and react in real time to both static data and streaming data from Apache Kafka, Kinesis, and HDF-based or cloud storage.
- **GraphX**
	- For manipulating graphs
