# Spark‚Äôs Distributed Execution
#Book
## Spark application 

Is the responsible for instantiating a `SparkSession`
![[Pasted image 20240105070659.png]]
## Spark Session:
![[C1. Fundamentals - Introduction#^a325d8]]
- Unified conduct to all Spark operations and data.
	- Dataset and DataFrame ‚Üí Using `SparkContext`
- Provides a **single unified entry point** to all Spark functionality
- Gives access to `SparkContext`
	- And other entry points such as => `SQLContext`, `HiveContext`, `SparkConf`, and `StreamingContext`
- In a stand along app the Spark Session can be created with any API languages
- In *Spark Shell* ‚Üí The application is created and accessed to the global variable `spark` ‚Üí Like in databricks

![[C1. Fundamentals - Introduction#^087066]]

![[Pasted image 20240105071413.png]]
## Spark Context:
- `SparkContext` ‚Üí The main entry point for spark functionality
- Holds a connection to the spark cluster manager
- Creates RDDs and broadcast variables in the cluster.
- Accesable through ‚Üí `SparkSession.sparkContext`

## Spark Driver:
- Part of the Spark application ‚Üí **Instantiates a** `SparkSession`
	- It communicates with the ==Cluster Manager==
		- Requests resources ‚Üí CPU, memory
		- Transforms the Spark operations into ==DAG computations==
- Responsible for orchestrating parallel operations on the spark cluster.
- Driver access the ==distributed components== in the cluster:
	- Spark executors
	- Cluster manager
- Coordinated in a `SparkContext`
	-  `SparkContext` make this coordination using the ==Driver==
	- In a standalone app ‚Üí responsible of executing `main()`
- Used to create DataFrames and RDDs

## Cluster Manager:
- The ==Master Node=== ‚Üí Cluster manager
	- manages the cluster environment and the servers that Spark will leverage to execute tasks.
	- Allocates resources to each application ‚Üí cluster nodes
- Supports five cluster managers:
	1. **Standalone** cluster manager: 
	2. Apache **Mesos** (a distributed system kernel)
	3. **Hadoop** YARN
	4. **Kubernetes**
	5. **Amazon** EC2

#### Deployment modes
- Spark can be deployed on some of the most popular environments ‚Üí Different configurations 

| Mode           | Spark Driver üèé                            | Executor ‚õë                                    | Cluster Manager üìê                                                                                                    |
| -------------- | ----------------------------------------- | --------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| Local          | Single JVM, like a laptop or single node  | Runs on the same JVM as the driver            | Runs on **the same host**                                                                                             |
| Standalone     | Can run on any node of the cluster        | Each node launch its **own** executor **JVM** | Allocated arbitrarily to any host in the cluster                                                                      |
| YARN (client)  | Runs on a client, not part of the cluster | YARN‚Äôs node manager container                 | YARN‚Äôs Resource Manager works with YARN‚Äôs application master to allocate the containers on nodeManagers for executors |
| YARN (cluster) | Runs on the YARN app master               | Same as YARN client                           | Same as YARN client                                                                                                   |
| Kubernetes     | Runs on a Kubernetes pod                  | Each worker runs within its own pod                                              | Kubernets Master                                                                                                                      |
## Workers and Spark Executors
- A **Spark Executor** runs on each **worker** node in the cluster.
	- Executors: 
		- Communicate with the driver program
		- Responsible for executing task in the Worker nodes
		- Only a single Executor runs per node
	- Worker:
		- Any node that can run programs in the cluster
![[C1. Fundamentals - Introduction#^9c1d86]]

# Distributed Data and partitions
