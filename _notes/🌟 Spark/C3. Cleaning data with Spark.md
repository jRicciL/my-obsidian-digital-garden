# Data Cleaning with Spark
#DataCamp 

## DataFrame Details

Process in data cleaning:
1. Reformatting or replacing text
2. Performing calculation
3. Removing garbage or incomplete data

Advantages of Spark:
- Scalable
- Powerful framework for data handling

### Spark Schemas
- Define the format of a DataFrame
	- Used to validate the data 
- May contain various data types:
	- Strings, dates, integers, arrays
- Can filter garbage data during import
- Improves read performance

#### Example Spark Schema
- Each `StructField` includes the name of the field, the data type, and whether it can include `null` entries.

```python
import pyspark.sql.types

peopleSchema = StructType([
	# Define the name field
	StructField('name', StringType(), True),
	# Add the age field
	StructField('age', IntegerType(), True),
	# Add the city field
	StructField('city', StringType(), True)
])
```

- Read `csv` file containing data using the ==Schema==

```python
people_df = spark.read.format('csv')\
			.load(name = 'rawdata.csv',
				  schema = peopleSchema)
```

### Immutability and Lazy Processing

- Spark DataFrames are immutable
	- Immutability is usually present in Functional programming
	- Defined once
	- Unable to be directly modified
	- Able to be shared efficiently
	- Recreated if reassigned

```python
# Load the CSV file
aa_dfw_df = spark.read.format('csv')\
				 .options(Header=True)\
				 .load('AA_DFW_2018.csv.gz')

# Add the airport column using the F.lower() method
aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))

# Drop the Destination Airport column
aa_dfw_df = aa_dfw_df.drop(
				aa_dfw_df['Destination Airport'])

# Show the DataFrame
aa_dfw_df.show()
```

### Understanding Parquet

Difficulties with CSV files

- No defined schema
- Nested data requires special handling
	- Identated data
- Encoding format limited
- Slow to parse in Spark
- If no `Schema` is defined =>
	- All data must be read before infer the data types
- Files cannot
- Predicate Pushdown√ë
	- files cannot be filtered
- Any intermediate use requires redefining schema

## The ==Parquet== Format
- A columnar data format developed for using in any Hadoop based system
	- Binary file format
- Supported in Spark and other data processing frameworks
- Supports predicate pushdown
- Automatically stores schema information
- Used to perform SQL operations
	- As backing stores for SparkSQL

### Working with Parquet
- Loading `Parquet` files

```python
# First option
df = spark.read.format('parquet').load(parquet_file)

# Second option
df = spark.read.paquet(parquet_file)
```

- Writing `Parquet` files

```python
# First option
df = spark.write.format('parquet').sav(parquet_file)

# Second option
df = spark.write.paquet(parquet_file)
```

### Parquet and SQL

```sql
spark.sql('''
		  SELECT *
		  FROM flights
		  WHERE flightduration < 100
		  ''')
```

#### Exercises

```python
# Read the Parquet file into flights_df
flights_df = spark.read.parquet('AA_DFW_ALL.parquet')

# Register the temp table
flights_df.createOrReplaceTempView('flights')

# Run a SQL query of the average flight duration
avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]
print('The average flight time is: %d' % avg_duration)python

```

## Manipulating DataFrames in the real world

DataFrames:
- Made up of rows and columns
- Immutable
- use various transformation operations

### Common DataFrame transformations
- `Filter`
	- Remove nulls
	- Remove odd entries
	- Split data from combined sources
	- Negate with `~`

![[Captura de Pantalla 2022-05-15 a la(s) 20.18.14.png]]

- `select()`
- `withColumn()`
- `drop()`

```python
# Return rows where name starts with "M"
voter_df.filter(
	voter_df.name.like('M%')
)

# Return name and position only
voters = voter_df.select('name', 'position')
```

### Column string transformations

- use `pyspark.sql.functions`

```python
import pyspark.sql.functions as F
```

![[Captura de Pantalla 2022-05-15 a la(s) 20.19.59.png]]

### `ArrayType()` column functions
- Various utility functions / transformations to interact with `ArrayType()`
- These are ==methods== that operate over columns:
	- `.size(<column>)` -> returns length of `arrayType()` column
	- `getItem(<index>)` -> used to retrieve a specific item at index of list column

#### Examples

```python
# Show the distinct VOTER_NAME entries
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)

# Filter voter_df where the VOTER_NAME is 1-20 characters in length
voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')

# Filter out voter_df where the VOTER_NAME contains an underscore
voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))

# Show the distinct VOTER_NAME entries again
voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)
```

```python
# Add a new column called splits separated on whitespace
voter_df = voter_df.withColumn('splits', 
                        F.split(voter_df.VOTER_NAME, '\s+'))

# Create a new column called first_name 
# based on the first item in splits
voter_df = voter_df.withColumn('first_name', 
                        voter_df.splits.getItem(0))

# Get the last entry of the splits list and 
# create a column called last_name
voter_df = voter_df.withColumn('last_name', 
                        voter_df.splits.getItem(
                            F.size('splits') - 1))

# Drop the splits column
voter_df = voter_df.drop('splits')

# Show the voter_df DataFrame
voter_df.show()
```

## Conditional DataFrame column operations

