---
---

# Sequence Models

- The sequence is very important to understand the meaning

A common ML algorithm:
![[Captura de Pantalla 2021-09-11 a la(s) 9.48.49.png]]

- But this does not take the ==sequence== into account=>
	- So, we should ask the model to understand the pattern behind the sequence.

![[Captura de Pantalla 2021-09-11 a la(s) 9.52.21.png]]

- This is similar to the basic idea of a Recurrent Neural Network #RNN 

![[Captura de Pantalla 2021-09-11 a la(s) 9.58.04.png]]


## LSTM => *Long short Term Memory*
#LSTM

- 🕐 LSTMS have an additional pipeline of contexts called ==cell state==.
	- Helps to ==keep context== from early tokens.
	- Cells states can also be ==bidirectional==.
		- Later context can impact earlier ones.

![[Captura de Pantalla 2021-09-11 a la(s) 10.01.34.png]]

***
# LSTM implementation

### Single-unit `LSTM`
- Using ==a single== LSTM unit with a `Bidirectional` layer.
```python
model = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		tokenizer.vocab_size, 
		64
	),
	# Bidirectional layer wrappes the LSTM layer
	tf.keras.layers.Bidirectional(
		# Bidirectional will double the 64 output
		tf.keras.layers.LSTM(64)
	),
	tf.keras.layers.Dense(64, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

#### Full code
Access to the [full code](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb)
```python
import tensorflow_datasets as tfds
import tensorflow as tf
print(tf.__version__)
```
Get the data
```python
# Get the data
dataset, info = tfds.load('imdb_reviews/subwords8k',
						 with_info = True,
						 as_supervised = True)

train_dataset, test_dataset = dataset['train'], dataset['test']
```
- 🔴  Deprecation issue with `TFDS datasets`:
![[Captura de Pantalla 2021-09-11 a la(s) 10.23.25.png]]


Define the `Tokenizer`
```python
# This dataset has been previously codified
# Subwords
tokenizer = info.features['text'].encoder
```
Data Loaders
```python
BUFFER_SIZE = 10_000
BATCH_SIZE = 64

train_dataset = train_dataset.shuffle(BUFFER_SIZE)
train_dataset = train_dataset.padded_batch(
	BATCH_SIZE,
	tf.compat.v1.data.gen_output_shapes(train_dataset)
)

test_dataset = test_dataset.padded_batch(
	BATCH_SIZE,
	tf.compat.v1.data.gen_output_shapes(test_dataset)
)
```

Model summary
```python
model.summary()
```
```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, 128)               66048     
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________

```

Define the MODEL
```python
model = tf.keras.Sequential([
	tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
	tf.keras.layers.Bidirectional(
		tf.keras.layers.LSTM(64)	
	),
	tf.keras.layers.Dense(64, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

Compile and fit the model
```python
model.compile(
	loss = 'binary_cross_entropy',
	optimizer = 'adam',
	metrics = ['accuracy']
)

history = model.fit(
	train_dataset,
	epochs = NUM_EPOCHS,
	validation_data = test_dataset
)
```

```python
Epoch 1/10 391/391 [==============================] - ETA: 0s - loss: 0.4849 - accuracy: 0.7681
```
- Training is very slow even when using a GPU

***
### Stacking  `LSTM` units
- Using ==stacking== #LSTM units
	- We need to use the `return_sequences = True` argument inside the `LSTM` unit =>
```python
model = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		tokenizer.vocab_size, 64
	),
	# Stacket Bidirectional LSTMS
	tf.keras.layers.Bidirectional(
		tf.keras.layers.LSTM(
			64,
			return_sequences = True
		)
	),
	tf.keras.layers.Bidirectional(
		tf.keras.layers.LSTM(32)
	),
	tf.keras.layers.Dense(64, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

#### Full code
Access to the [full code](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W3/ungraded_labs/C3_W3_Lab_2_multiple_layer_LSTM.ipynb)

![[Captura de Pantalla 2021-09-11 a la(s) 19.28.50.png]]