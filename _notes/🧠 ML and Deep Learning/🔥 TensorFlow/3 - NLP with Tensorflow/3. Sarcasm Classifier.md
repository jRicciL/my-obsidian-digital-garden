---
---

# Sarcasm Classifier

- Do the imports
```python
import json 
import tensorflow as tf 

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessin.sequence import pad_sequences
```

- Se the hyperparameters
```python
vocab_size = 10_000
embedding_sim = 16
max_length = 32
trunc_type = 'post'
padding_type = '<OOV>'
training_size = 20_000
```

- Download the #sarcasm data
```python
!wget --no-check-certificate \
	https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \
	-O /tmp/sarcasm.json
```

- Load the data as an iterable
```python
with open('/tmp/sarcasm.json', 'r') as f:
	datastore = json.load(f)
	
sentences = []
labels = []

for item in datastore:
	sentences.append(item['headline'])
	lables.append(item['is_sarcastic'])
```

- Training and test sets

```python
# Need this block to get it to work with TensorFlow 2.x
import numpy as np
training_padded = np.array(training_padded)
training_labels = np.array(training_labels)
testing_padded  = np.array(testing_padded)
testing_labels  = np.array(testing_labels
```

```python
# Training set
tarining_sentences = sentences[0:training_size]
training_labels    = labesl[0:training_size]
# Testing sets
testing_sentences = sentences[training_size:]
testing_labels    = labesl[training_size:]
```

- Build the tokenizer and sequence the texts
```python
# Define the Tokenizer using the training sequences
tokenizer = Tokenizer(
	num_words = vocab_size,
    oov_token = oov_tok)
# fit on the training set
tokenizer.fit_on_texts(training_sentences)

# Get the word index for futher exploration
word_index = tokenizer.word_index

# Sequence the texts and perform padding
# Training sequences
training_sequences = tokenizer.texts_to_sequences(training_sequences)
training_padded = pad_sequences(training_sequences,
							    maxlen = max_length,
							   paddin = paddint_type,
							   truncating = trunc_type)
# Testing sequences 
testing_sequences = tokenizer.texts_to_sequences(testing_sequences)
testing_padded = pad_sequences(testing_sequences,
							  maxlen = max_length,
							  padding = padding_type,
							  truncating = trunc_type)
```

- Define the Neural network
```python
model = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		input_dim    = vocab_size,
		output_dim   = embedding_dim,
		input_length = max_length
	),
	tf.keras.layers.GlobalAveragePooling1D(),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])

model.compile(loss = 'binary_crossentropy',
			  optimizer = 'adam', 
			 metrics = ['accuracy'])

model.summary()
```

```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 100, 16)           160000    
_________________________________________________________________
global_average_pooling1d (Gl (None, 16)                0         
_________________________________________________________________
dense (Dense)                (None, 24)                408       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 160,433
Trainable params: 160,433
Non-trainable params: 0
_________________________________________________________________ 

CÃ³digoTexto
```
- Perform the training
```python
num_epochs = 30

history = model.fit(
	training_padded,
	trianing_labels,
	epochs = num_epochs,
    validation_data = (testing_padded, testing_labels),
	verbose = 2
)
```