---
---

# GRU vs LSTM vs CNN

```python
import tensorflow as tf
import tensorflow_datasets as tfds
```

```python
import numpy as np

imbd, info = tfds.load("imdb_reviews",
					   with_info=True, 
					   as_supervised=True)

train_data, test_data = imdb['train'], imdb['test']

training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = []

# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()
for s,l in train_data:
  training_sentences.append(str(s.numpy()))
  training_labels.append(l.numpy())
  
for s,l in test_data:
  training_sentences.append(str(s.numpy()))
  testing_labels.append(l.numpy())
  
training_labels_final = np.array(training_labels)
testing_labels_final = np.array(testing_labels)
```

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = "<OOV>"

# Perform the sequencing and padding
tokenizer = Tokenizer(num_words = vocab_size, 
					  oov_token = oov_tok)
tokenizer.fit_on_texts(trainin_sentences)

word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(training_sentences)
padded = pad_sequences(sequences,
					  maxlen = max_length,
					  truncating = trunc_type)

testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,
							  maxlen = max_length,
							  truncating = trunc_type)
```

Inspec the word index:
```python
reverse_word_index = dict([
	(value, key) for (key, value) in word_index.items()
])

def decode_review(text):
	return ' '.join([reverse_word_index.get(i, '?')
					 for i in text])
```

## Flatten model
```python
flat_mol = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		input_dim = vocab_size,
		output_dim = embedding_dim,
		input_length = max_length
	),
	tf.keras.layers.Flatten(),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

- Model summary:
	- ==171,533== parameters
	- It uses a `Flatten()` layer
	- 5 seconds per epoch
```python
Model: "Flatten"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           160000    
_________________________________________________________________
flatten_1                    (None, 1920)              0     
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 11526       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,533
Trainable params: 171,533
Non-trainable params: 0
_________________________________________________________________
```
![[Captura de Pantalla 2021-09-12 a la(s) 18.39.07.png]]
## LSTM model
```python
lstm = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		input_dim = vocab_size,
		output_dim = embedding_dim,
		input_length = max_length
	),
	tf.keras.layers.Bidirectional(
		tf.keras.layers.LSTM(32)
	),
	tf.keras.Dense(6, activation = 'relu'),
	tf.keras.Dense(1, activation = 'sigmoid')
])
lstm.summary()
```
- Model summary:
	- ==172,941== parameters
	- It uses a `Bidirectional()` layer
```python
Model: "LSTM"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           160000    
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                12544     
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 390       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 7         
=================================================================
Total params: 172,941
Trainable params: 172,941
Non-trainable params: 0
_________________________________________________________________
```
![[Captura de Pantalla 2021-09-12 a la(s) 18.42.12.png]]

## CNN model
```python

cnn = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		input_dim = vocab_size,
		output_dim = embedding_dim,
		input_length = max_length
	),
	tf.keras.layers.GlobalAveragePooling1D(),
	tf.keras.layers.Conv1D(128, 5, activation = 'relu'),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
cnn.summary()
```
- Model summary:
	- ==171, 149== parameters
	- Kernel size => 5
	- `128` kernels/filters
	- It uses a `GlobalAveragePooling1D()`

```python
Model: "CNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 120, 16)           160000    
_________________________________________________________________
conv1d (Conv1D)              (None, 116, 128)          10368     
_________________________________________________________________
global_average_pooling1d (Gl (None, 128)               0         
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 774       
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,149
Trainable params: 171,149
Non-trainable params: 0
_________________________________________________________________

```
![[Captura de Pantalla 2021-09-12 a la(s) 18.45.03.png]]

## GRU
#GRU
-> 
```python
gru = tf.keras.Sequential([
	tf.keras.layers.Embedding(
		input_size = vocab_size,
		output_size = embedding_dim,
		input_length = max_length
	),
	tf.keras.layers.Bidirectional(
		tf.keras.layers.GRU(32)
	),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])

model.summary()
```
- Model summary:
	- ==169,997== parameters
```python
Model: "GRU"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                9600      
_________________________________________________________________
dense (Dense)                (None, 6)                 390       
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 169,997
Trainable params: 169,997
Non-trainable params: 0

```
![[Captura de Pantalla 2021-09-12 a la(s) 18.44.25.png]]