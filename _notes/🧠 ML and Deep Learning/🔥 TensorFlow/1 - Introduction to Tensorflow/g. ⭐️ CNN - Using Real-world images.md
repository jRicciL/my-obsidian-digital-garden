# CNN - Using real-worlds images

## ImageDataGenerator

#ImageDataGenerator

### Understand #ImageDataGenerator

- Use a ==directory structure== to perform the #train-test splitting with autogenerated labels
	- 📂 `Images`
		- 📂 `Trainig`
			- 😸 `class 1` -> label 1
			- 🐶 `class 2` -> label 2
		- 📂 `Validation`
			- 😸 `class 1` -> label 1
			- 🐶 `class 2` -> label 2

![[Pasted image 20220223214240.png]]

### Keras implementation

- Use `ImageDataGenerator`
	- Use the method `.flow_from_directory()`
	- Use `class_mode = 'binary'` for **binary** classification

#### Data Preprocessing

- Download the dataset
```python
!wget --no-check-certificate \ 
	https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \ 
	-O /tmp/horse-or-human.zi
```

- `Unzip` the dataset
```python
import os
import zipfile

local_zip = '/tmp/horse-or-human.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp/horse-or-human')
zip_ref.close()
```

#### Training set

```python
from tensorflow.keras.preprocessing.image \
		import ImageDataGenerator

# Normalize the data using `rescale`
train_datagen = ImageDataGenerator(rescale = 1. / 255)

# TRAINING
# Instantiate the train_generator
training_generator = train_datagen.flow_from_directory(
	# Point at the directory
	train_dir,
	# Resize the image to have the same size
	target_size = (300, 300),
	# Train and validation will be do in batches
	batch_size = 128,
	class_mode = 'binary'
)
```

#### Validation set

```python
test_datagen = ImageDataGenerator(rescale = 1. / 255)

validation_generator = test_datagen.flow_from_directory(
	validation_dir,
	target_size = (300, 300),
	batch_size = 32,
	class_mode = 'binary'
)
```

### Define the CNN

- Check [[e. ⭐️ Standard CNN - Keras]] and [[e. ⭐️ Intro to CNNs - Fashion MNIST]]

- This architecture was used in the #Tensorflow Developer Course 1

```python
import tensorflow as tf

IMG_WIDTH = 300
IMG_HEIGHT = 300
input_shape = (IMG_WIDTH, IMG_HEIGHT)

model = tf.keras.models.Sequential([
		# This is the first convolution
    tf.keras.layers.Conv2D(16, (3,3), 
							activation='relu', 
							input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(32, (3,3), 
							activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tf.keras.layers.Conv2D(64, (3,3), 
							activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tf.keras.layers.Conv2D(64, (3,3), 
							activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fifth convolution
    tf.keras.layers.Conv2D(64, (3,3), 
							activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, 
							activation='relu'),
    # Only 1 output neuron. It will contain a value 
	# from 0-1 where 0 for 1 class ('horses') 
	# and 1 for the other ('humans')
    tf.keras.layers.Dense(1, 
							activation='sigmoid')
])
```


## Fitting using `.fit_generator()`

### Compile the model
Model compile commonly includes:

-   `loss` → Loss function
-   `optimizer` → Method of [[Optimizers]] → GD related methods
	-   We will use #RMSProp 
-   `metrics` → Array of metrics

```python
from tensorflow.keras.optimizers import RMSprop 

model.compile( 
	loss = 'binary_crossentropy', 
	optimizer = RMSprop(lr = 0.001), 
	metrics = ['acc'] )
```

### Training the model with `fit_generator()`
- 🚨 ==NOTE== 🚨 => `model.fit()` in #Tensorflow 2 now supports generators

- Below we will use `.fit_generator()`
	-   `generator` ⇒ Train generator
	-   `steps_per_epoch` ⇒ Total number of steps before declaring one epoch finished and starting the next
	-   `epochs` ⇒ Number of epochs
	-   `validation_data` ⇒ Data on which validate the loss and model metrics → can be a `generator`
	-   `validation_steps` ⇒ Only relevant if `validation_data` is provided.
		-   Total number of steps (batches samples) to draw before stopping when performing validation at the end of every epoch.

##### Using `.fit_generator()`

```python
history = model.fit_generator(
	generator = train_generator,
	steps_per_epoch = 8, 
	epochs = 15,
	validation_data = validation_generator,
	validation_steps = 8,
	verbose = 2
)
```

## Make prediction
- Use `image` from `keras.preprocessing`

```python
import numpy as np
from google.colab import files
from keras.preprocessing import image

# use this tool to select how to upload the dataset
uploaded = files.upload()

# Iterate over all images
for fn in uploaded.keys():
	# Predicting images
	path = '/content/' + fn
	# Load the image using the target_size
	img = image.load_img(
		path,
		target_size = (300, 300)
	)
	# Get the array values and add an 
	# extra dim at the begining
	x = image.img_to_array(img)
	x = np.expand_dims(x, axis = 0)
	
	images = np.vstack([x])
	classes = model.predict(image, batch_size = 10)
	print(classes[0])
	if classes[0] > 0.5:
		print(fn + " is a human")
	else:
		print(fn + " is a horese")
	
```