---
---

# RNN for time series with adjusted Learning Rate

```python
import tensorflow as tf
import numpy as np
import matplitlib.pyplot as plt
```

### Some miscellaneous function
- Some plotting functions to create a time series

```python
def plot_series(time, series, format="-", start=0, end=None):
    plt.plot(time[start:end], series[start:end], format)
    plt.xlabel("Time")
    plt.ylabel("Value")
    plt.grid(True)

def trend(time, slope=0):
    return slope * time

def seasonal_pattern(season_time):
    """Just an arbitrary pattern, you can change it if you wish"""
    return np.where(season_time < 0.4,
                    np.cos(season_time * 2 * np.pi),
                    1 / np.exp(3 * season_time))

def seasonality(time, period, amplitude=1, phase=0):
    """Repeats the same pattern at each period"""
    season_time = ((time + phase) % period) / period
    return amplitude * seasonal_pattern(season_time)

def noise(time, noise_level=1, seed=None):
    rnd = np.random.RandomState(seed)
    return rnd.randn(len(time)) * noise_level
```

## Create the time series
```python
time      = np.arange(4 * 365 + 1, dtype="float32")
baseline  = 10
series    = trend(time, 0.1)  
baseline  = 10
amplitude = 40
slope     = 0.05
noise_level = 5

# Create the series
series = baseline + trend(time, slope) + \
		 seasonality(time, period = 365, 
					 amplitude = amplitude)
# Update with noise
series += noise(time, noise_level, seed=42)

split_time = 1000
time_train = time[:split_time]
x_train    = series[:split_time]
time_valid = time[split_time:]
x_valid    = series[split_time:]

window_size = 20
batch_size  = 32
shuffle_buffer_size = 1000
```

## Create a windowed dataset using `TensorFlow`
1. Use `from_tensor_slices(series)`
2. Use the `window` method to create the windows
3. Use a `flat_map` function to capture the `w-1` values as $X$ and the last value as $y$
4. Shuffle the list of pair values `(x, y)`
5. Most dataset input pipelines should end with a call to `prefetch`

```python
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
	# Dataset from tensor
	dataset = tf.data.Dataset.from_tensor_slices(series)
	# Create the windows
	dataset = dataset.window(window_size + 1, shift = 1, drop_remainer = True)
	# Use a map function to capture the w -1 values as X
	# and the last value as y
	dataset = datast.flat_map(
		lambda window: window.batch(window_size + 1)
	)
	dataset = dataset.shuffle(shuffle_buffer)\.
		map(lambda window: (window[:-1], window[-1]))
	# Most dataset input pipelines should end with a call to `prefetch`
	dataset = dataset.batch(batch_size).prefetch(1)
	
	return dataset
```

## Create the model

- Clear the session and define some random seeds 
```python
tf.keras.backed.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
```

- Create the **train set** using the `windowed_dataset()` function.

```python
train_set = windowed_dataset(
	x_train,
	window_size,
	batch_size = 128,
	shuffle_buffer = shuffle_buffer_size
)
```

- Define the model

```python
model = tf.keras.models.Sequential([
	tf.keras.layers.Lambda(
		lambda x: tf.expand_dims(x, axis = -1),
		input_shape = [None]
	),
	tf.keras.layers.SimpleRNN(40, return_sequences = True),
	tf.keras.layers.simpleRNN(40),
	tf.keras.layers.Dense(1),
	tf.keras.layers.Lambda(lambda x: x * 100.0)
])
```

## Learning rate Scheduler
- Create a ==Learning Rate== scheduler.

```python
lr_schedule = tf.keras.callbacks.LearningRateSchedurler(
	lambda epoch: 1e-8 * 10**(epoch / 20)	
)
```

- Define the optimizer

```python
optimizer = tf.keras.optimizers.SGD(
	learning_rate = 1e-8, momentum = 0.9
)
```

- Compile the model

```python
model.compile(
	loss = tf.keras.losses.Huber(),
	optimizer = optimizer,
	metrics = ['mse']
)
```

- Train the model to find the best value of #learning-rate 

```python
history = model.fit(train_set, 
					epochs = 100, 
					callbacks = [lr_schedule])
```

### Plot the `lr` vs the `loss`

```python
plt.figure(figsize = (15, 7))
plt.semilogx(history.history["lr"], 
			 history.history["loss"])
plt.title('Learning Rate Scheduler')
plt.xlabel('Learning rate')
plt.ylabel('Loss')
plt.axis([1e-8, 1e-4, 0, 30])
```
![[Pasted image 20220218182041.png]]

- The ==best value== seems to be between `10e-6` and `10e-5` =>
	- Therefore, we will select `5e-5`

## Train the model using the best `learning_rate`

```python
# Clear the session
tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)

# Instantiate the dataset
dataset = windowed_dataset(x_train, window_size, batch_size=128, shuffle_buffer=shuffle_buffer_size)


# Instantiate the model
model = tf.keras.models.Sequential([
  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                      input_shape=[None]),
  tf.keras.layers.SimpleRNN(40, return_sequences=True),
  tf.keras.layers.SimpleRNN(40),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 100.0)
])

# Define the optimizer using the selected learning rate
optimizer = tf.keras.optimizers.SGD(
						learning_rate=5e-5, 
						momentum=0.9)

# Compile and train the model
model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(dataset,epochs=400
```

## Forecast

```python
forecast = []

# Make the prediction for each window
for time in range(len(series) - window_size):
	forecast.append(
		model.predict(
			series[time:time + window_size][np.newaxis]
		)
	)

# Use `split_time` - `window_size` to consider only the
# fraction of the series belonging to the test set
forecast = forecast[split_time-window_size:]
results = np.array(forecast)[:, 0, 0]

```

![[Pasted image 20220218184016.png]]

##  Compute the Loss across the epochs

- Compute the #MSE 

```python
tf.keras.metrics.mean_absolute_error(
	X_valid, results
).numpy()
```

- Compute the #MSE 
```python
tf.keras.metrics.mean_squared_error(
	X_valid, results
).numpy()
```

### Plot the `loss` and `MAE` across the epochs

```python
import matplotlib.image  as mpimg
import matplotlib.pyplot as plt

#-----------------------------------------------------------
# Retrieve a list of list results on training and test data
# sets for each training epoch
#-----------------------------------------------------------
mae=history.history['mae']
loss=history.history['loss']

epochs=range(len(loss)) # Get number of epochs

#------------------------------------------------
# Plot MAE and Loss
#------------------------------------------------
plt.figure(figsize = (15,5))
plt.plot(epochs, mae, 'r')
plt.plot(epochs, loss, 'b')
plt.title('MAE and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["MAE", "Loss"])

plt.figure(figsize = (15,5))

epochs_zoom = epochs[200:]
mae_zoom = mae[200:]
loss_zoom = loss[200:]

#------------------------------------------------
# Plot Zoomed MAE and Loss
#------------------------------------------------
plt.plot(epochs_zoom, mae_zoom, 'r')
plt.plot(epochs_zoom, loss_zoom, 'b')
plt.title('MAE and Loss')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["MAE", "Loss"])

plt.figure(figsize = (15,5))
```

![[Pasted image 20220218185254.png]]

### Zoom to last epochs

![[Pasted image 20220218185307.png]]