---
---

# Gated Recurrent Networks 

- Proposed by Cho, *et al.* (2014)^[Kyunghyun Cho et al., “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation,” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (2014): 1724–1734.]
- #GRN is a modification of the common #RNN layer ->
	- This allows to capturing long-range connections and helps with #vanishingGradients^[[4. W1 - Vanishing Gradients with RNN]]

![[Pasted image 20211226102531.png|550]]

### The RNN unit

$$a^{<t>} = g(W_a[a^{<t-1}, x^{<t>}] + b_a)$$
$$\hat y = g'(a^{<t>})$$
- where $g$ => `tanh`
- where $g'$ => `softmax`

![[Captura de Pantalla 2021-12-26 a la(s) 10.26.04.png|500]]
### GRN unit 
#### => Simplified version with a single Gate

- $C^t$ => ==memory cell== at the position $t$
- For #GRU => $a^t = C^t$

**Therefore, $C^t$ is computed as follows**

1. First, we have to compute a ==candidate value== $C$, denoted as $\tilde C^t$, from the previous value $\tilde C^{t-1}$ as follows: 

$$\tilde C^t = tanh(Wa[C^{t-1}, x^t] + b_a)$$
2. Second, we have to compute a ==Gate== (update Gate):
$$\Gamma_u = \sigma(W_u[C^{t-1}, x^t] + b_u)$$
3. Third, compute $C^t$:
$$C^t = (\Gamma _u \otimes \tilde C) +( [1 - \Gamma _u] \otimes C^{t -1})$$
## Full version of GRU unit
- This version includes a ==new Gate== => $\Gamma_r$
0. $C^t = a^t$
1. $\Gamma _r = \sigma(W_r[C^{t-1}, x^t] + b_r)$ => ==Reset== or ==relevant== gate
2. $\Gamma_u = \sigma(W_u[C^{t-1}, x^t] + b_u)$ => ==Update=== gate
3. $\tilde C^t = tanh(W_c[\Gamma_r \otimes C^{t-1}, x^t] + b_c)$ 
3. $C^t = (\Gamma _u \otimes \tilde C) +( [1 - \Gamma _u] \otimes C^{t -1})$

#### Other versions from other sources

![[Captura de Pantalla 2021-12-26 a la(s) 11.55.30.png]]

- The GRU cell is a simplified version of the LSTM cell.
	- It seem to preform just as well

## GRU Highlights

