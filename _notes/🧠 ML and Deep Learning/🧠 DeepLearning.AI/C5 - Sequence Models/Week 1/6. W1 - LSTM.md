---
---

# Long-term short memory


- **Details about the Gates and States of the LSTM unit ->** [[11. W1 - Assignment - LSTM]]
- **Keras implementations**:
	1. [[7. GRU-LSTM-and-CNN]]

***
## Introduction
- Proposed originally in 1997 by Sepp Hochreiter and Jurgen Schmidhuber ->
	- It gradually improve over the years
- LSTM are faster than RNN and deal with long-term dependencies

## The LSTM unit
Let's start from the #GRU unit:
- ![[5. W1 - GRN#Full version of GRU unit]]
### LSTM unit
- Differently from #GRU units:
	- $a^t$ is ==not equal== to $C^t$
	- Do not include the $\Gamma_r$ gate, used to compute $\tilde C^t$
	- It **includes three  gates**:
		- ==Update Gate==: $\Gamma_u = \sigma(W_u[a^{t-1}, x^t] + b_u)$
		- ==Forget Gate==: $\Gamma_f = \sigma(W_f[a^{t-1}, x^t] + b_f)$
		- ==Output Gate==: $\Gamma_o = \sigma(W_o[a^{t-1}, x^t] + b_o)$
	- They are computed similarly => But they have they own parameters $W$
- Then, $C^t$ is computed as:
	- $C^t = (\Gamma _u \otimes \tilde C) +(  \Gamma _f\otimes C^{t -1})$
- Finally, $a^t$ is computed as:
	- $a^t = \Gamma _o \otimes than( C^t)$

### Peephole connections

- In a regular `LSTM` cell, the gate controllers can look only at the input $x^t$ and the previous short-term state $a^{t -1}.
- One idea is to give them a bit more context by letting them peek at the long-term state.
	- This idea was proposed in 2000 by Felix Gers and Jurgen Schmidhuber
	- The idea includes extra connections named **_peephole connections_**
	- The previous long-term state $C^{t-1}$ is added to the ==Update and Forget== gate controllers:
		-  ==Update Gate==: $\Gamma_u = \sigma(W_u[a^{t-1}, C^{t-1}] + b_u)$
		- ==Forget Gate==: $\Gamma_f = \sigma(W_f[a^{t-1}, C^{t-1}] + b_f)$
	- And the current long-term state is added as input to the controller of the output gate.
		- ==Output Gate==: $\Gamma_o = \sigma(W_o[a^{t-1}, C^t] + b_o)$


