---
---

# Word2vec

#word2vect #word-embedding 

## Continuous Bag of Words
- The #CBOW model learns to predict a target word in its neighborhood, ==using all words.==

## Skip-Gram model
The key problem of #SkipGram is the computation of the #softmax function

1. **A simpler and computationally efficient algorithm to learn word embeddings.**
2. Randomly pick a word to be the ==context== word related to the ==target== word.
	- Do this for multiple pairs
3. Check the following example:
	- Randomly pick a word to be the **context** word ($c$) -> `orange`:
	- <mark style='background-color: #FFA793 !important'>Note</mark>: Avoid #Stop_words and very frequent words or use a *probability distribution* function.
		- *I want a glass of ==orange== juice to go along with my cereal*
	- Randomly pick a word, within a window of size $w$, to be the **target** word ($t$) -> `glass`
		- *I want a ==glass== of orange juice to go along with my cereal*
4. As a result we could have **multiple pair** of *context*/*target* words:

|Context | Target |
|---|---|
|orange|juice|
|orange|glass|
|orange|my|

5. The words are included in a vocabulary $V$:
	- The words are #One-hot-encoded => A word $w$ is encoded as $O_w$
	
6. We take the **embedding matrix** $E$ to get the embedding vectors $e_w$
	- The embedding of $c$ => $E\cdotO_c = e_c$

5. In the **model**:
	- The $c$ word (`context`) is the predictive variable $x$
		- Coded as $O_c$
	- The $t$ word (`target`) is the response/target variable $y$
	- The model is implemented as a Classification model

6. We use a #softmax as the activation function to predict $\hat y$
	- **Softmax** function: 
$$p(t|c) = e^{\Theta^T_t} e_c / \sum^{|V|}_{j=1}e^{\Theta^T_j} e_c$$
- where $\Theta_t$ is the parameter associated with an output $t$

7. The loss function will be:

$$\mathcal{L}(\hat y, y) = - \sum^{|V|}_{j=1}y_i \mathrm{log}\   {\hat y_i}$$
- However, there are some problems with #softmax classificaiton
- During the computation of $p(t|c)$ the sum is very expensive and has to be computed $|V|$ times
- An alternative is to use a [[Hierarchical Softmax Function]] function, which takes only $log|V|$

***

## Negative Sampling

- Similar to the #SkipGram model, but **more efficient** -> It uses multiple ==negative examples== to train $|V|$ binary classifiers.

### Defining a new learning problem
1. For a **context** $c$ word randomly chosen:
	1. get a **target** word $t$ as the ==positive example== 
	2. Get a set of $k$ **negative examples**
		- So we have a $k$ to $1$ ratio of negative/positive examples
- We will have:


| Context | Word  | Target? |
| ------- | ----- | ------- |
| orange  | juice | 1       |
| orange  | king  | 0       |
| orange  | book  | 0       |
| ...     | ...   | 0       |
| orange  | of    | 0       |

![[Captura de Pantalla 2022-01-04 a la(s) 12.01.07.png]]

2. Use *Context + word* as the $x$ variable and *Target?* as the $y$ variable
	- $k$ = 5 to 20 for small datasets
	- $k$ = 2 - 5 for larger datasets

3. **Train** a **supervised model** to map $x$ -> $y$

4. Given the $(x,y)$ dataset => Implement a [[Logistic Regression]] model to estimate the probability of $y =1$ given the pair $(c,t)$:

$$p(y = 1 | c, t) = \sigma(\Theta^T_t e_c)$$
- <mark style='background-color: #9CE684 !important'>Important!</mark>: As a result, we will have $|V|$ logistic regression classifiers.
	- If $|V| = 10,000$ we will have $10,000$ binary classification problems
	- This is <mark style='background-color: #FFA793 !important'>cheaper</mark> than the #SkipGram implementation

![[Captura de Pantalla 2022-01-04 a la(s) 12.09.18.png]]

### Selecting negative examples

- <mark style='background-color: #9CE684 !important'>First option:</mark> Sample according to the **empirical frequency** of the words inside the corpus.
	- ðŸ”´ But the problem is the over-representation of words like `the`, `of`, etc.
- <mark style='background-color: #9CE684 !important'>Second option:</mark> Sample uniformly random.
	- ðŸ”´ But this does not represent the distribution of words
- <mark style='background-color: #FFA793 !important'>Third option:</mark> 
	- An heuristic formula that combines the two above options:
		- $p(w_i) = \frac{f(w_i)^{3/4}} {\sum^{|V|}_j f(w_j)^{3/4}}$
		- where $f(w_i)$ is the observed distribution of the word $w_i$
	- This is a better approach frequently used.


## References
- Mikolov, et al., 2013. Efficient estimation of word representations in vector space.