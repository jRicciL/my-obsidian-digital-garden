---
---

# Learning Word Embeddings

When learning word embeddings:
- we create an artificial task of estimating $P(target|context)$.
- It is okay if we do poorly on this artificial prediction task.
- The more important by-product of this task is that we learn a useful set of word embeddings.

## Neural Language Model

1. Given a sentence like:
	- `I want a glass of orange ____`
	- And a vocabulary $V$ of size $|V|$ -> example $|V| = 10,000$
2. Use the **Embedding Matrix**^[[[15. W2 - Introduction to Word Embeddings#Embedding Matrix]]] $E$ and multiply it to the #One-hot-encoded vector $O_w$ of the word $w$ to get the embedding vector $e_w$
3. Do the previous step for each word in the sentence
4. **Feed** all word embeddings into a neural network
	- Stack the embedding vectors
	- The dimension will depend on the number of words and the embedding size $a_n$
		- A common approach to have a **fixed** number of words is use a window of size $k$
		- Then, the size of the stacked vector fed to the network will be $a_n \times k$
5. **Train** the neural network using a #softmax function
	- **Predict** which of the $10,000$ words is the most probable to appear at the blank part `____` of the sentence
	- #Backpropagation will find the best parameters to get the best $w$ to fill the black (`___`) in order to maximize the $similarity$ function.


![[Captura de Pantalla 2022-01-04 a la(s) 10.26.24.png]]


### Other context/target pairs

- Use #n-grams modeling => [[NLP_Feature_Engineering]]

![[Captura de Pantalla 2022-01-04 a la(s) 10.31.48.png]]

***

## References
- [Bengio, et al., 2003. A neural probabilistic language model.](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)