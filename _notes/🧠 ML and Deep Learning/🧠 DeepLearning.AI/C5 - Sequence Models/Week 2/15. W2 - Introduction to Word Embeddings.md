---
---

# Introduction to Word Embedding

- Used for building #NLP^[[[introduction_to_natural_lenguage_processing]]] applications

## Word representation

A vocabulary $V$ of size $|V| = n$. 
- $V=$ [a, aaron, zulu, `<UNK>`]

### One-hot encoding representation
Each word is #One-hot-encoded:

- $Man = [0,0,0, ..., 1, ..., 0]$
- Suppose it has $1$ at the `5391` position
- This is represented as $O_5391$

#### Issues related to 1-hot representation
- You cannot capture the relationship between  #One-hot-encoded  words
	- Any product between two 1-hot encoded vectors is $0$
	- Therefore, it is not possible to distance between any pari of the vectors
- So, it is not possible to get similar words closser

### Featurized representation => Word Embedding

- Notation => For word embedding the vector $e_k$ represents a given word.
	- $e_k$ has a dimension $\mathbb{R}^p$
	- Each dimension is a better representation of the word and tries to approximate to a real-world feature.
		- Such as *gender*, *age*, *size*, ...
- More about word [[2. Embeddings]]

#### Visualizing Word Embeddings
- A common algorithm for visualization is #t-SNE


## Using word Embeddings
- Used for [[Named Entity Recognition]]
- #TODO: Fill the list with other applications

### Transfer learning and word embeddings

1. **Learn word embeddings** from large text corpus (1-100 billions of words)
	- [[16. W2 - Learning Word Embeddings]]
2. **Transfer embedding** to new task with smaller training set
3. Optional: Continue to **finetune** the roed word with new data.

***

## Properties of Word Embeddings

### Analogies

- `Man` --> `Woman`
- `King` --> `Queen`

We will have:

$$e_{man} - e_{woman} \approx e_{king} - e_{queen}$$

 ![[Captura de Pantalla 2022-01-04 a la(s) 7.26.09.png]]

 - Proposed by [Mikolov, et al., 2013, Linguistic regularities in continuous space word representations](https://aclanthology.org/N13-1090.pdf)

### Implementation

Find word $w$ that satisfies: 

$$\underset{w}{\mathrm{argmin}} \ \mathrm{sim}(e_w, e_{king} - e_{man} + e_{woman})$$

where $sim$ is a similarity function:

#### Similarity function

- **Cosine Similarity** (#CosineSimilarity):
	- More often used

$$sim(u, v) = \frac{u^T v}{||u||_2 \cdot ||v||_2}$$
- **Square difference**:
	- A disimilarity function

$$dis(u,v) = ||u- v||^2$$

- This is a paralelogram relationship:

![[Captura de Pantalla 2022-01-04 a la(s) 7.46.41.png]]

## Embedding Matrix

![[Captura de Pantalla 2022-01-04 a la(s) 7.53.11.png]]

