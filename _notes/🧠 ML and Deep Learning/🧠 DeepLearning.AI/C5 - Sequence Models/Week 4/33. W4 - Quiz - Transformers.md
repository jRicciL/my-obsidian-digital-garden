---
---

# Transformers

1. A Transformer Network, like ist predecessors #RNN, #GRU, and #LSTM, can process inofrmation one word at a time.
	- `False`: A transformer network can ingest entire sentences all at the same timw

2. Transformer Network methodology is taken:
	- [x] Convolutional Neural Network style of processing
	- [x] Attention mechanism

3. The Concept of *Self-attention* is that:
	- ![[Pasted image 20220125221550.png]]
	- Given a word, its neighbouring words are used to compute its context by selecting the ==summingup== the word values to map the Attention related to that given word.

4. Which of the following correctly represents **Attention**?
	-	Attention(Q,K,V)=softmax(dk​QKT​)V

5. Are the following statements true regarding Query ($Q$), Key ($K$), and Value ($V$)?
- Q = interesting questions about the words in a sentence
- K = specific representations of words given a Q
- V = qualities of words given a Q
-  -> `False`: *Q = interesting questions about the words in a sentence, K = qualities of words given a Q, V = specific representations of words given a Q*

6. ![[Pasted image 20220125222017.png]]

$i$ here represetns the computed attention weight matrix associated with the $ith$ *word* in a sentence.
- `False`: $i$ represents the computed attention weight matrix associated with the $i$ ==head==

7. Following the architecture withtin a Transformer Network (without displaying positional encoding and output layers)

![[Pasted image 20220125222149.png]]

What information does the *Decoder* take from the *Encoder* for its second block of **Multi-head Attention**? (Marked $X$ pointed by the independent arrow)
- ==Answer==: $V$ and $K$

8. Following the architecture withtin a Transformer Network (without displaying positional encoding and output layers)

![[Pasted image 20220125222446.png]]

What is the output layers of the *Decoder*? (Marker Y)
- ==Answer==: Linear layer followed by a Softmax layer

9. Why is the positional encoding important in the translation process?
	- [x] Position and word order are essential in sentence construction of any language.
	- [x] Providing extra information to our model

10. Which of these is a good criteria for a good positional encoding algorithm?
	- [x] It should output a unique encoding for each time-step (word's position in a sentence)
	- [x] Distance between any two time-steps should be consistent for all sentence lengths  
	- [x] The algorithm should be able to generalize to longer sentences.