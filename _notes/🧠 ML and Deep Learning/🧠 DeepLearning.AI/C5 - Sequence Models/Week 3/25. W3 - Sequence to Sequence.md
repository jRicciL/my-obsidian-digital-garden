---
---

# Sequence to Sequence Model

***

## Basic Model
### Use an `Encoder`/`Decoder` Architecture

`Input` --> `Output`

$X \rightarrow y$

- The idea is to use an ==Encoder== network and a ==Decoder== network to process the `input` sentence and the `output` sentence.
![[Captura de Pantalla 2022-01-15 a la(s) 11.20.37.png]]


### Image captioning
- A very similar architecture works for **Image captioning**:

1. Use a classical [[1. W1 - CNN]] network architecture (like those mentioned here [[5. W2 - Classic CNN]]).
	- THis network will encode the image features
	- This will be the ==Encoder network==
2. Get rid of the #softmax activation function
3. Feed an [[2. W1 - Recurrent Neural Network]]
	1. The #RNN will generate the captioning of the image.
	2. Works really well for short captioning

## Picking the most likely sentence

### Machine translation as building a conditional language model

#### **Language model:**
- What it does is to estimate the probability of a sentence.
	- $P(y^1, y^2, ..., y^t)$
- Used to generate novel sentences

#### **Machine translation model:**
- It models the probability of an `ouput` sentence (==a given language==) conditioned on the `input` sentence (==a target language==).
	- It is a <mark style='background-color: #FFA793 !important'>Conditional language model</mark>
	- $P(y^1, y^2, ..., y^{Ty} | x^1, x^2, ..., x^{Tx})$
- We want to find the sentence $Y$ that maximizes the *conditional* probability given $X$
	- And algorithm that actually find the value of $Y$ that maximizes:
		- $\underset{y^1, ..., y^{Ty}} {\mathrm{argmax}} ~ P(y^1, y^2, ..., y^{Ty} | X)$
		- The join probability of the whole sequence $Y$
- ðŸŸ  It has an `Encoder` network
	- This network generates starting values for the `Decoder` network instead of always starting along with the vector of all zeros --> Like with the *Language model* architecture.
- ðŸŸ¢ It has a `Decoder` network
	- It's relatively similar to the *Language Model* architecture

#### Why not a greedy search?
- ==Greedy search==:
	- is an algorithm from computer science which says to *generate the first word* just pick whatever is the most likely first word *according to your conditional language model*.
- This **approach does not work because** 
	- It is <mark style='background-color: #FFA793 !important'>not optimal</mark> to chose one word at time.
	- it is a consecutive process where the $t$ word is chosen based on the probability of the previously $t-1$ chosen words.