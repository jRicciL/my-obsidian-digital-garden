---
---

# Attention Model

## Attention Model Intuition

> The problem of long sequences:

![[Captura de Pantalla 2022-01-21 a la(s) 7.12.38.png]]

The ==Encoder/Decoder== based model (using the #BleuScore and the #BeamAlgorithm) works quite well with relatively short sentences.
But it fails with larger sentences.

![[Captura de Pantalla 2022-01-21 a la(s) 7.14.22.png]]

- The attention model uses ==attention weights== to indicate the level of attention the model should put to each word inside a sentence.

## Deeper into the Attention model ðŸ¤”

- Start with a **Bidirectional** #LSTM to compute the features -> ==Pre-attention== RNN
- The `input` sentence (*French*) has $T'$ words, with each word appearing at its respective time step $t'$

![[Captura de Pantalla 2022-01-21 a la(s) 7.23.41.png]]

- The `output` sentence (the translation) is generated by a forward #RNN -> ==Post-attention== #RNN
- At each time-step $t$, the Decoder has an input **context** $C$
	- $C$ will depend on the attention parameters $\alpha^{<t, t'>}$
	- The sum of all $\alpha^{<1, t'>}$ should be $1$
		- The equation of $\alpha^{<1, t'>}$ is similar to the #softmax function.
		- $\alpha^{<t, t'>} = \mathbf{exp}(e^{<t,t'>}) / \sum_{t'=1}^{T'} \mathbf{exp}(e^{<t,t'>})$
- $C$ is computed as:
	- $C^t = \sum_{t'} \alpha^{<t, t'>} a^{t'}$
	- where $a^t'$ is the encoded vector obtained from the `input` sentences at time step $t'$

![[Captura de Pantalla 2022-01-21 a la(s) 7.29.51.png]]

- <mark style='background-color: #FFA793 !important'>ImportantðŸ”¥</mark>: $\alpha^{<t, t'>}$ is the amount of attention $y^t$ should pay to $a^{t'}$ 

![[Captura de Pantalla 2022-01-21 a la(s) 7.35.34.png]]

## Visualization of the attention weights 

![[Pasted image 20220121074448.png]]


## References
- Bahdanau, et al. 2014. [Neural Machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)