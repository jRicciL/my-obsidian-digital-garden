---
---

# Inception networks

## The intuition behind the Inception network
Try multiple filters or even multiple types of layers:
- Let the network chose whatever the combinations of filter sizes or types of networks are better for the prediction


![[Captura de Pantalla 2021-11-23 a la(s) 19.42.07.png]]

#### Computationally expensive
- <mark style='background-color: #FFA793 !important'>Problem</mark> => Computational cost
- ![[Captura de Pantalla 2021-11-23 a la(s) 19.50.11.png]]

#### How to reduce the computational cost?

One alternative to reduce the computational cost is use `1x1` ==convolutions== to *reduce the depth of the volumes* => Reduce the number of channels
- The above will reduce the number of computations and assure to get the same output volume.
- This is called the <mark style='background-color: #9CE684 !important'>Bottle neck layer</mark>: #BottleneckLayer => using `1x1` convolution layer
- Does not seem to hurt the performance of the model

- ![[Captura de Pantalla 2021-11-23 a la(s) 19.49.26.png]]

## The Inception network

### The inception blocks
![[Captura de Pantalla 2021-11-23 a la(s) 20.07.38.png]]

The image below shows the inception network built from  <mark style='background-color: #FFA793 !important'>inception blocks</mark>

![[Captura de Pantalla 2021-11-23 a la(s) 20.09.59.png]]

### Side branches for predictions
 Additionally, the inception network has extra branches:
 - with Dense layers and Output (`softmax`) layers to make predictions at a given level of the network.
 - The idea is to regularize the network => reduce the complexity of the predictions at the end of the network

This network comes from Google => ==GoogleNet==
![[Captura de Pantalla 2021-11-23 a la(s) 20.13.19.png]]