---
---

# Setting up your goal

## Single Number evaluation metric
- Use a single real number evaluation metric to get insights about the models performance => Quickly!!
	- `F1-score`: *Harmonic mean*
	- `accuracy`
	- `R^2`
	- `AUC-ROC`: #AUC-ROC
	- `AUC-PR`

#### Performance Metrics for Classification

<div class="rich-link-card-container"><a class="rich-link-card" href="https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc" target="_blank">
	<div class="rich-link-image-container">
		<div class="rich-link-image" style="background-image: url('https://neptune.ai/wp-content/uploads/F1-ROC-AUC-featured.png')">
	</div>
	</div>
	<div class="rich-link-card-text">
		<h1 class="rich-link-card-title">F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose? - neptune.ai</h1>
		<p class="rich-link-card-description">
		PR AUC and F1 Score are very robust evaluation metrics that work great for many classification problems but from my experience more commonly used metrics are Accuracy and ROC AUC. Are they better? Not really. As with the famous ‚ÄúAUC vs Accuracy‚Äù discussion: there are real benefits to using both. The big question is when.¬† [‚Ä¶]
		</p>
		<p class="rich-link-href">
		https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc
		</p>
	</div>
</a></div>

***
## Satisficing and Optimizing metrics

It is about using an ==Optimizing== metric along with an ==Satisficing== metric.

- üîµ  Pick **one metric** to be _** <mark style='background-color: #93EBFF !important'>Optimized</mark> **_
- üü¢  The **rest** of the metrics to be _**<mark style='background-color: #9CE684 !important'>Satisficing</mark> **_

![[Captura de Pantalla 2021-09-12 a la(s) 14.58.07.png]]

***
## Setting `Train`/`Dev`/`Test` sets

### Source of the `dev`/`test` sets
- Try that the `dev` and `test` sets come from the same distribution.
- Choose a `dev` set and `test` set to reflect data you expect to get in the future and consider important to do well on.

![[Captura de Pantalla 2021-09-12 a la(s) 15.05.50.png]]

### Size of the `Dev` and `Test` sets

#### Old way of splitting data
- => 70 - 30
- ==> 60, 20, 20

Only for small datasets: 
- Less than `10,000` observations

![[Captura de Pantalla 2021-09-12 a la(s) 15.09.11.png]]

#### With larger datasets
- => 99, 1, 1
![[Captura de Pantalla 2021-09-12 a la(s) 15.09.42.png]]

#### Size of test set
- Set the `test` set to be big enough to give high confidence in the overall performance of the system.

## When to Change `Dev`/`Test` sets and metrics?

> If doing well on your metric + `dev`/`test` set does not correspond to doing well on your application, change your metric and/or `dev`/`test` set