---
---

# Deep Convolutional Models

## LeNet -5
#CNN-LeNet

- 1998
- Its goal was to recognize handwritten digits
- It did not use padding
- Images dims => `32x32x1`
	- Gray scale images
- Architecture:
	- Input layer =>
		-  $32 \times 32 \times 1$
	- First CNN layer =>
		- filters => `6`
		- Kernel: `5x5`, stride = `1`
		- $28 \times 28 \times 6$
	- Average pooling: `2x2`, s = `2`
		- $14 \times 14 \times 6$
	- Second CNN layer =>
		- filters => `16`
		- kernel: `5x5`, stride = `1`
		- $10 \times 10 \times 16$
	- Average pooling: `2x2`, s = `2`
		- $5 \times 5 \times 6$
	- Flatten layer => dims: `(400, )`
	- First Dense layer
		- 120 units
	- Second Dense layer
		- 84 units
	- Output  layer `softmax`
		- 10 units

![[Captura de Pantalla 2021-11-21 a la(s) 12.28.40.png]]
- It originally did not use ReLU


#### Code (Keras)
```python
import tensorflow.keras.layers as tfl
from tensorflow.keras import Sequential

lenet5 = Sequential([
    tfl.Input((32,32,1)),
    tfl.Conv2D(filters = 6,
               kernel_size = (5,5),
               strides = 1,
               activation = 'relu'),
    tfl.AveragePooling2D(pool_size=(2,2), 
                         strides = (2)),
    tfl.Conv2D(filters = 16,
               kernel_size = (5,5),
               activation = 'relu'),
    tfl.AveragePooling2D((2,2), strides=(2)),
    tfl.Flatten(),
    tfl.Dense(120, activation='relu'),
    tfl.Dense(84, activation = 'relu'),
    tfl.Dense(10, activation = 'softmax')

])

print(lenet5.summary())
visualkeras.layered_view(model = lenet5, scale_xy = 4)
```

```
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d_5 (Conv2D)           (None, 28, 28, 6)         156       
                                                                 
 average_pooling2d_4 (Averag  (None, 14, 14, 6)        0         
 ePooling2D)                                                     
                                                                 
 conv2d_6 (Conv2D)           (None, 10, 10, 16)        2416      
                                                                 
 average_pooling2d_5 (Averag  (None, 5, 5, 16)         0         
 ePooling2D)                                                     
                                                                 
 flatten (Flatten)           (None, 400)               0         
                                                                 
 dense_6 (Dense)             (None, 120)               48120     
                                                                 
 dense_7 (Dense)             (None, 84)                10164     
                                                                 
 dense_8 (Dense)             (None, 10)                850       
                                                                 
=================================================================
Total params: 61,706
Trainable params: 61,706
Non-trainable params: 0
```

![[descarga.png]]

#### Patterns from LeNet-5
- 60 K parameters
- The width and height decreases as the number of channels increase
- Conv layers are followed by a Pooling layer


***
## AlexNet

- Named after Alex Krizhevsky -> First author of the paper
- Input shape => `227x227x3`
	- Although the paper mentions `224` instead


![[Captura de Pantalla 2021-11-21 a la(s) 12.57.45.png]]

##### VisualKeras Version
```python
from PIL import ImageFont

font = ImageFont.truetype(
	"/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf", 32)
visualkeras.layered_view(model = AlexNet, 
						 scale_xy = 50,legend=True, 
						 font = font, type_ignore=[Activation, Dropout])
```
![[descarga (2).png]]



#### Architecture:
- Input layer =>
	-  $227 \times 227 \times 3$
- First CNN layer =>
	- filters => `96`
	- Kernel: `11x11`, stride = `4`
	- $55 \times 55 \times 96$
- Max pooling: `3x3`, s = `2`
	- $27 \times 27 \times 96$
- Second CNN layer (Same) =>
	- filters => `256`
	- kernel: `5x5`,  padding = `same`
	- $27 \times 27 \times 256$
- Max pooling: `3x3`, s = `2`
	- $13 \times 13 \times 256$
- Third CNN layer (Same) =>
	- filters => `384`
	- kernel: `3x3`,  padding = `same`
	- $13 \times 13 \times 384$
- Fourth CNN layer (Same) =>
	- filters => `384`
	- kernel: `3x3`,  padding = `same`
	- $13 \times 13 \times 384$
- Fifth CNN layer (Same) =>
	- filters => `256`
	- kernel: `3x3`,  padding = `same`
	- $13 \times 13 \times 256$
- Max pooling: `3x3`, s = `2`
	- $6 \times 6 \times 256$
- Flatten layer => dims: `(9216, )`
- First Dense layer
	- 4096 units
- Second Dense layer
	- 4096 units
- Output  layer `softmax`
	- 1000 units

#### Patters from AlexNet
- 60 M of parameters
- It uses ReLU as the activation function
- It originally had a Local Response Normalization layer..
	- But this idea was then removed

#### Code
```python
import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
import numpy as np

np.random.seed(1000)
 #Instantiation
AlexNet = Sequential()

#1st Convolutional Layer
AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#2nd Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#3rd Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

#4th Convolutional Layer
AlexNet.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))

#5th Convolutional Layer
AlexNet.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
AlexNet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))

#Passing it to a Fully Connected layer
AlexNet.add(Flatten())
# 1st Fully Connected Layer
AlexNet.add(Dense(4096, input_shape=(32,32,3,)))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
# Add Dropout to prevent overfitting
AlexNet.add(Dropout(0.4))

#2nd Fully Connected Layer
AlexNet.add(Dense(4096))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
#Add Dropout
AlexNet.add(Dropout(0.4))

#3rd Fully Connected Layer
AlexNet.add(Dense(1000))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('relu'))
#Add Dropout
AlexNet.add(Dropout(0.4))

#Output Layer
AlexNet.add(Dense(10))
AlexNet.add(BatchNormalization())
AlexNet.add(Activation('softmax'))

#Model Summary
AlexNet.summary()
```


***
## VGG-16
#VGG16

- It has 16 layers (with trainable weights)
- It simplifies the CNN architectures
	- A very uniform architecture
	- The number of filters doubles at each block of layers
- Always uses:
	- CONV of `3x3` filters, `same` padding
	- MaxPool `2x2`, s = `2`
- Combine consecutive CNN layers
- It has 138M parameters
- #VGG19 is a bigger version of this network

![[Captura de Pantalla 2021-11-21 a la(s) 13.19.52.png]]

##### VisualKeras Version
```python
from PIL import ImageFont

font = ImageFont.truetype(
	"/usr/share/fonts/truetype/humor-sans/Humor-Sans.ttf", 32)
visualkeras.layered_view(model = VGG16, 
						 scale_xy = 50,legend=True, 
						 font = font, type_ignore=[Activation, Dropout])
```

![[descarga (3).png]]

#### Code
```python
# VGG16
from keras.layers import Input, MaxPool2D, Conv2D, Dense


model = Sequential()
model.add(Input((224,224,3)))

model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(Conv2D(filters=64,kernel_size=(3,3),padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=128, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=256, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(Conv2D(filters=512, kernel_size=(3,3), padding="same", activation="relu"))
model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))

# Classification
model.add(Flatten())
model.add(Dense(units=4096,activation="relu"))
model.add(Dense(units=4096,activation="relu"))

# Output layer
model.add(Dense(units=1000, activation="softmax"))

```