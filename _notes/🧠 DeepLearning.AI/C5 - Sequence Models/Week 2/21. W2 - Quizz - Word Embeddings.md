---
---

# Quiz - Word Embeddings

#quiz #Coursera 

## Natural Language Processing and Word Embeddings

1. Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words.
	- `False`

2. What is #t-SNE?
	- A non-linear dimensionality reduction technique

3. Suppose you download a pre-trained word embedding which has been trained on a huge corpus of text. You then use this word embedding to train an RNN for a language task of recognizing if someone is happy from a short snippet of text, using a small training set.
	a. Then even if the word “ecstatic” does not appear in your small training set, your RNN might reasonably be expected to recognize “I’m ecstatic” as deserving a label y = 1y=1.
	- `True`

4. Which of these equations do you think should hold for a good word embeddint?
- [x] $e_{boy} - e_{brother} \approx e_{girl} - e_{sister}$
- [x] $e_{boy} - e_{girl} \approx e_{brother} - e_{sister}$

5. Let $E$ be an embedding matrix, and let $O_{1234}$ be a one-hot vector corresponding to word 1234. Then to get the embedding of word 1234, why don't we call $E*O_{1234}$:
	- ==It is computationally wasteful.==
	-  $E \in \mathbb{R}^{|V|\times a}$  and $O_w \in \mathbb{R}^{a}$ 

6. When learning word embeddings, we create an artificial task of estimating $P(target | context)$. It is okay if we do poorly on this artificial prediction tas: the more important by-product of this task is that we learn a useful set of word embeddingso
- `True`

7. In the #word2vect algorithm, you estimate $P(t|c), where $t$ is the target word and $c$ is the context word. How are $t$ and $c$ **chosen from the training set?** ==Pick the best answer==
	- $t$ and $c$ are chosen to be nearby words

8. Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings. The #word2vect model uses the following #softmax function:
	- $P(t|c) = \frac{e^{\Theta^T_t \cdot e_{c}}} {\sum_i^{10000} e^{\Theta^T_i \cdot e_{c}}}$
	- which of these statements are correct?
		- [] After training, we should expect $\Theta_t$ to be very close to $e_c$ when $t$ and $c$ are the same word
		- [x] $\theta_t$ and $e_c$ are both trained with an optimization alforithm such as [[Optimizers#ADAM]] or [[Optimizers#Gradient Descent]].
		- [x] $\theta_t$ and $e_c$ are both 500 dimensional vectors

9. Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings. The [[18. W2 - GloVe]] model minimizes this objective:
	
- Which of these statements are correct?
- [x] $X_ij$ is the number of times word $j$ appears in the context of word $i$
- [x] $\theta_i$ and $e_j$ should be initialized randomly at the beginning of training
- [x] The weighting function $f(.)$ must satisfy $f(0) = 0$

10. You have trained a word embeddings using a text dataset of $m_1$ words. You are considering using these word embeddings for a language task, for which you have a separate labeled dataset of $m_2$ words. Keeping in mind that using word embeddings is a form of transfer learning, under which of htese circumstances would you expect the word embeddings to be helpful?
	- $m_1>>m_2$