---
---

# Beam Search
#Sequence2Sequence
#BeamAlgorithm
***

- The algorithm has a parameter $B$ -> ==beam width==
	- When $B=3$ the algorithm considers the best $3$ options of each word $y$ at time $t$
	- It will keep track of those selected words.
	- This means, there will be $B$ instances (copies) of the network to evaluate each word.
- If $B=1$ we will end with a **greedy search** implementation.
	
![[Captura de Pantalla 2022-01-15 a la(s) 12.11.32.png]]
	
- The ==Beam search algorithm== then selects the best $t+1$ word given each of the top $B$ words selected at time $t$
- Then we will evaluate:
	- $P(y^t, y^{t+1}|x) = P(y^t|x)P(y^1, y^{t+1}|x)$
	- This give us the probability of the $t$ and $t+1$ words given $X$
- The same process is repeated for the other top $B$ words.

![[Captura de Pantalla 2022-01-15 a la(s) 12.22.10.png]]

As a result, ==Beam search== maximizes this probability:

$$\underset{y}{\mathrm{argmax}}  \prod ^ {T_y} _{t=1}  P(y^t|x, y^1,...,y^{t-1})$$

## Refinements to Beam Search

### Summation of `log` probabilities
Instead of maximazing this product, which will result in very small values:
![[Captura de Pantalla 2022-01-15 a la(s) 12.32.05.png]]

We use a ==logarithmic== version, and thus maximize the following **summation**:
$$\underset{y}{\mathrm{argmax}}  \sum ^ {T_y}_{t=1}  \mathrm{log} P(y^t|x, y^1,...,y^{t-1})$$

### Length normalization
- ðŸ”´ The length of the sentences <mark style='background-color: #FFA793 !important'>affect</mark> the $P$ value:
	- Because shorter sentences give higher probabilities
	- This occurs due to the equation above, that implies the multiplication of $t$ individual probabilities.

One alternative is to normalize the value of the last equation by dividing the summation by the number of words in $y$: $T_y$

$$\frac{1}{T_y^{\alpha}} \underset{y}{\mathrm{argmax}}  \sum ^ {T_y}_{t=1}  \mathrm{log} P(y^t|x, y^1,...,y^{t-1})$$
- where $\alpha$ is an hyperparameter to *soft* the normalization.

### How to chose the value of $B$
- If $B$ is very large:
	- Slower implementation
	- Better results
- If $B$ is small:
	- Faster implementation
	- Worse results
- In production systems is common to see values around $10$ and $100$


## Error Analysis on Beam Search

#BeamAlgorithm Search is a heuristic search algorithm -> It is an approximate search algorithm.

### What if Beam Search makes a mistake

**Example**:
==Original sentence:==
- *Jane visite l'Afrique en septembre* <- *French*

==Human translation== => $\star y$
- *Jane visits Africa in September* <- *English*

==The model translation==
- *Jane visited Africa last September*

The model has two main components:
1. An #RNN model
2. The Beam Search algorithm

There are two possible source of errors:

Case 1:
- $P(\star y|x) > P(\hat y| x)$
	- Beam search chose $\hat y$, but $\star y$ attains higher $P(y|x)$.
	- **Conclusion**: Beam search is a fault

Case 2:
- $P(\star y|x) \le P(\hat y| x)$
- $\star y$ is better translation that $\hat y$ but RNN predicted $P(\star y| x) P(\hat y| x)$
- **Conclusion**: #RNN model is at fault

## Bleu Score
A method for automatic evaluation of machine translation
#BleuScore

- How to determine which is the best translation if there are multiple of very good translations for a given sentence?

The #BleuScore uses the following:
- Reference target sentences in the training set
- #n-grams 
- A Modified #Precision
- A Brevity penalty