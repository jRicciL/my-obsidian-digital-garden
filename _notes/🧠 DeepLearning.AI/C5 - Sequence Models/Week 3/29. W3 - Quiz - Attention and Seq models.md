---
---

# Quiz: Sequence Models and Attention Mechanism

#quiz 

1. Consider using this encoder-decoder model for machine translation
![[Pasted image 20220121233405.png]]
This models is a "conditional language model" in the sense that the encoder portion (shown in green) is modeling the probability of the input sentence $x$
	- `false`

2. In [[26. W3 - Beam Search]], if you increase the beam width $B$, which of the following would you expect to be true?

- [x] Beam search will be use up more memory
- [x] Beam search will generally find better solutions
- [x] Beam search will run more slowly
- [ ] Beam search will converge after fewer steps.

3. In machine translation, if we carry out [[26. W3 - Beam Search]] without using sentence normalization, the algorithm will tend to output overly short translations.
	- `True`

4. Suppose you are building a speech recognition system, which uses an RNN model to map from audio clip $x$ to a text transcript $y$. Your algorithm uses beam search to try to find the vaue of $y$ that maximizes $P(y|x)$
   On a dev set example, given an input audio clip, your algorithm outputs the transcript $\hat y$ = “I’m building an A Eye system in Silly con Valley.”, whereas a human gives a much superior transcript $y^*$ = “I’m building an AI system in Silicon Valley.”
   
   -   No, because $P(y^* \mid x) \leq P(\hat{y} \mid x)$ $indicates the error should be attributed to the RNN rather than to the search algorithm.
 
 5. Continuing the example from Q4, suppose you work on your algorithm for a few more weeks, and now find that for the vast majority of examples on which your algorithm makes a mistake,  $P(y^* \mid x) > P(\hat{y} \mid x)$. This suggests you should focus your attention on improving the search algorithm.
 	- `True`

6. Considering the attention model for machine translation.

![[Pasted image 20220121234724.png]]

Further, here is the formula for $\alpha^{<t,t'>}$

![[Pasted image 20220121234755.png]]

Which of the following statements about $\alpha^{<t,t'>}$ is true?
- [x] The sum of all $\alpha^{<t,t'>}$ , over $t'$ is equal to 1.
- [x] We expect $\alpha^{<t,t’>}$ to be generally larger for values of $a^{<t’>}$ that are highly relevant to the value the network should output for $y^{<t>}$. (Note the indices in the superscripts.)

7. The network learns where to 'pay attention' by learning values $e^t$, which are computed using a small neural network: We can't replace $s^{<t-1>}$ with $s^{<t>}$ as an input to this neural network. This is because $s^{<t>}$ depends on $\alpha^{<t,t’>}$ which in turn depends on $e^{<t,t’>}$; so at the time we need to evaluate this network, we haven’t computed $s^{<t>}$ yet.
	
	- `True`

8. Compared to the encoder-decoder model shown in Question 1 of this quiz, we expect the attention model to have the greates advantage when:
	- The input sequence length $T_x$ is large

9. Under the CTC model, identical repeated characters not separated by the “blank” character (\_) are collapsed. Under the CTC model, what does the following string collapse to?
__c_oo_o_kk___b_ooooo__oo__kkk

	- `cookbook`

10. In #TriggerWordDetection $x^t$ is:
- Features of the audio (such as spectrogram features) at time $t$.