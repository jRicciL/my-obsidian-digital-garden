---
---

# Recurrent Neural Network

## Why not a standard network?

![[Captura de Pantalla 2021-12-21 a la(s) 21.11.12.png]]

**Problems**:
- Inputs, outputs can be different lengths in different examples.
- Doesn't share features learned across different positions of text.
	- *it is important to somehow encode information about the word ordering more directly within the architecture of the network*
- A better representation could be used to reduced the number of parameters
![[Captura de Pantalla 2021-12-21 a la(s) 21.51.06.png]]

## Recurrent Neural Networks
Two main desiderata for the processing of sequences include:
1. The ability to receive and process `inputs` in the same order as they are present in the sequence
2. The treatment of `inputs` at each time-stamp in a similar manner in relation to previous history of input.
3. Has the ability to compute a function of variable-legth inputs

#### Basic architecture

- The network includes an `input` => Which is a #One-hot-encoding token
- A `hidden representation` => $h_t$, also denoted as the activation $a_t$ or $a^{<t>}$
- A `self loop` (shown in *a)*) with $Whh$ (or $Waa$) parameters
- A `prediction` = $\hat y$ 
![[Captura de Pantalla 2021-12-21 a la(s) 21.22.29.png]]
- In practice it is possible for either the input or the output units to be missing an any particular time stamp.
![[Captura de Pantalla 2021-12-21 a la(s) 22.32.46.png]]

<mark style='background-color: #9CE684 !important'>Important!</mark>: The *weight* matrices are shared across different temporal layers.
- Thus, the same parameters are used for each `input`
- This ensures that the same function is used at each time-stamp

The ==time-layered== representation makes sense because in practice the sequence are of finite length.
- This way the network looks more like a feed-forward network
 
#### Notes about RNN
 - Assuming the above representation -> The information flows from left to right => `Unidictional`
	 - It only uses the information that is early in the sequence.
	 - It uses one input for every position in the sequence.

### Forward propagation
![[Captura de Pantalla 2021-12-21 a la(s) 22.46.19.png]]
The input vector at time $t$ is $xt$ with a hidden state $h_t$ with an output vector $y_t$:
- The hidden vector is $p$-dimensional => $p$ regulates the complexity of the embedding.
- The activations (or hidden states) are a function of the input vector at time $t$ and the hidden vector at time $t-1$:
	- $a_0$ (or $h_0$) is commonly generated randomly or using zeros.
		- Because there is no input from the hidden layer at the beginning of a sentence
	- $a_t$, with $t>0$, is computed by:
		- $a_t = g(W_{aa}a_{t-1} + W_{ax} + b_a)$ <- $g$ usually is `tanh`
		- The hyperbolic tangent function (`tanh`) is used to deal with the #ExplodingGradients problem 
			- Because `tanh` is a saturating activation function.
	- $y_t =  g(W_{ay}a_t + b_y)$ <- $g$ used to be `sigmoid` or `softmax`
		- It has the same dimensionality as the lexicon.
	- The same weights $W_{xa}$, $W_{aa}$, $W_{ay}$, are used for all the time-stamps.
		- These will remain fixed after the neural network has been trained
	- For the hidden vector, the `than` and `ReLU` activations are more common.
		- The activation function is applied element wise

## Backpropagation through time

### The loss function
$L(\hat y_t, y_t) = - y_t log \hat y_t - (1 - y_t)log(1 - \hat y_y)$

- Backpropagation goes from right to left.
	- ==Backpropagation through time==

# Different types of RNN
- Modify the #RNN architecture to address each of the data types according to the desired `input` and `outputs`
![[Captura de Pantalla 2021-12-22 a la(s) 20.25.46.png]]

### Examples of #RNN architectures and its applications

#### Classification
==Many to one==
- Only one `output` at the last loop, at the last time step of the `input`

#### Music generation
==One to many==
- Just one `input` and as many `outputs` as desired/required
- Used for sequence generation

#### Many-to-many with different lengths
- For machine translation
- Used where the `input` and the `output` sequences do not have the same length.
- It has **two distinct parts** ==> A `encoder` and a `decoder`.
	
![[Captura de Pantalla 2021-12-26 a la(s) 11.20.23.png|500]]![[Captura de Pantalla 2021-12-22 a la(s) 20.53.53.png]]