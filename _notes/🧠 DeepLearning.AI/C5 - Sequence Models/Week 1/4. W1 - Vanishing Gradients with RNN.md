---
---

# Vanishing Gradients
#vanishingGradients 

***

Language can have very long term dependencies =>
An example on predicting either the word `was ` or `were`
- **Seresto** the cat, which Jaky and me really love too much, ==was== running on the table.
- **Seresto** and **Tikiki**, which are really crazy and also really fat and big, ==were== sleeping on the couch.

It the sentence is very long -> The network will be very wide
- This will cause #vanishingGradients problems.
- Therefore the network will not ==memorize== the very early tokes ->
	- IT WILL FORGET THE INITIAL INFORMATION
	- The last `ouputs` will be not affected by the very early `inputs`
- The newtork will gradually forget the first inputs in the sequence.

![[Captura de Pantalla 2021-12-26 a la(s) 8.17.52.png]]

#### Exploding gradients 
#ExplodingGradients is easier to solve than #vanishingGradients 
- It is observed as the generationg of very large values in the `gradient vectors` => Resulting into numerical overflow => `NaN` values.
- Can be ==solve== by using **Gradient Clipping**:
	- Given some `threshold` (maximum value) -> Rescale the gradient vectors

- However, #vanishingGradients is not as easy to solve as #ExplodingGradients 

