---
---

# Language model and sequence generation

### Training a RNN for sequence generation

Ideally, we would like the probability of the next word to be predicted correctly from the probabilities of the previous words.
![[Captura de Pantalla 2021-12-25 a la(s) 8.31.33.png]]
### Terms
- Tokenize
- Corpus
- Lexicon
	- Number of tokens of size $n$
	- The `input` vector is #One-hot-encoding, meaning that at time $t$ it is $n$-dimensional 
- `<EOS>` => End of sentence
- `<START>` => Start token
- `<UNK>` => Unknown sentence

![[Captura de Pantalla 2021-12-22 a la(s) 21.05.15.png]]

## Sampling nobel Sequences
After training a Sequence Model -> How to sample novel sequences?

- Sample for a given distribution to generate the novel sequences.
	- At the first time-step choose the first word based on the lexicon probability distribution -> Gives => $\hat y_1$ 
	- At the next $t$ time-steps -> Choose the next word based on the previous word -> $\hat y_{t-1}$  is the `input` and $\hat y_t$  will be the `output`
![[Captura de Pantalla 2021-12-26 a la(s) 8.05.22.png]]

