---
---

# RNN Quiz

1. *Suppose your training examples are sentences (sequences of words). Which of the following refers to the $j^{th}$ word in the $i^{th}$ training example?*
	- $x^{(i)<j>}$


2. Consider this RNN:
 ![[Pasted image 20211226125755.png]]

 This specific type of architecture is appropriate when:
 - $T_x = T_y$

3. *To which of these tasks would you apply a many-to-one RNN architecture? (Check all that apply).*
- ![[Pasted image 20211226130039.png]]
	- Sentiment classification
	- Gender recognition from speech

4. *You are training this RNN language model:*
	- ![[Pasted image 20211226130132.png]]
	- At the $t$ time step, what is the #RNN doing?
		- ![[Captura de Pantalla 2021-12-26 a la(s) 13.03.17.png]]
5. *You have finished training a language model RNN and are using it to sample random sentences, as follows:*
	1. ![[Pasted image 20211226130355.png]]
	- *What are you doing at each time step $t$?*
	- Random sample a word using the RNN estimated probabilites
	- Then pass this selected word to the next time-step

6. *You are training a #RNN, and find that your weights and activation are all taking on the value on `NaN` => Which of these is the most likely cause of this problem?*
	- Exploding Gradient Problem => #ExplodingGradients 

7. Suppose you are training a #LSTM. You have a `10000` word vocabulary, and are using a #LSTM with 100-dimensional activations $a^t$. What is the dimension of $\Gamma_u$ at each time step?
	- `100`

8. Here are the update equations for the #GRU 
	- ![[Pasted image 20211226132233.png]]
	- *Alice proposes to simplify the GRU by always removing the $\Gamma_u$​. I.e., setting \Gamma_uΓu​ = 1. Betty proposes to simplify the GRU by removing the \Gamma_rΓr​. I. e., setting \Gamma_rΓr​ = 1 always. Which of these models is more likely to work without vanishing gradient problems even when trained on very long input sequences?*
	- ![[Captura de Pantalla 2021-12-26 a la(s) 13.27.50.png]]

9. Here are the equations for the GRU and LSTM
*From these, we can see that the Update Gate and Forget Gate in the LSTM play a role similar to __ and __ in the GRU. What should go in the blanks?*
- $\Gamma _u$ and $(1 - \Gamma _u)$

10. *You have a pet dog whose mood is heavily dependent on the current and past few days’ weather. You’ve collected data for the past 365 days on the weather, which you represent as a sequence as x^{<1>}, …, x^{<365>}x<1>,…,x<365>. You’ve also collected data on your dog’s mood, which you represent as y^{<1>}, …, y^{<365>}y<1>,…,y<365>. You’d like to build a model to map from x \rightarrow yx→y. Should you use a Unidirectional RNN or Bidirectional RNN for this problem?*
- 