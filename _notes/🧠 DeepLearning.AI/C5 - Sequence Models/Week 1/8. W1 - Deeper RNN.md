---
---

# Deeper RNN

- You can stack different units of #RNN, #GRU, or #LSTM 
- Usually ==three== levels is enough:
	- They are computationally expensive
![[Captura de Pantalla 2021-12-26 a la(s) 12.48.03.png]]

#### #Keras implementation

```python
# Simple RNN implementation
model = keras.models.Sequential([
	keras.layers.SimpleRNN(20, 
						   return_sequences = True,
						   input_shape = [None, 1]),
	keras.layers.SimpleRNN(20,
						   return_sequences = True),
	# Just return the last output
	# Used for a binary classification problem
	keras.layers.SimpleRNN(1)
])
```

>  - Make sure to set `return_sequences=True` for all recurrent layers (except the last one, if you only care about the last output).
>  - If you donâ€™t, they will output a 2D array (containing only the output of the last time step) instead of a 3D array (containing outputs for all time steps), and the next recurrent layer will complain that you are not feeding it sequences in the expected 3D format.