---
---

# Text Generative models and literature
***
So, this time the next word in a sentence will be the target variable

```python
tokenizer = Tokenizer()

data = 'In the town of ...\nSentence two\nSentence three'
# Split the whole text into multiple lines
corpus = data.lower().split('\n')

# Tokenize
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1
```


## Training the data
- For each sentence we will create a set of different `n-grams`:
```python
input_sequences = []
for line in corpus:
	# For each line in the corpus
	token_list = tokenizer.text_to_sequences([line])[0]
	# Create the n-grmas
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[ : i +1]
		input_sequences.append(n_gram_sequence)
```
- The result
![[Captura de Pantalla 2021-09-21 a la(s) 20.58.19.png]]

- Find the length of the longest sentence in the corpus
```python
max_sequence_len = max([len(x) for x in input_sequences])
```

#### Padding the sequences
- Pad all the sequences so they all will be of the same length
- We are still working with n-grams:
	- So the examples below and above are the same sentence but with different number of words

```python
input_sequences = np.array(
	pad_sequences(input_sequences, 
				  maxlen = max_sequence_len,
				  padding = 'pre')
)
```

![[Captura de Pantalla 2021-09-21 a la(s) 21.02.51.png]]

#### Featurization
- $X$ (all characters but the last) and $y$ (last character)
	
![[Captura de Pantalla 2021-09-21 a la(s) 21.12.08.png]]

Each time the next observation includes more words from the due to a bigger n-gram

- Now do it in python
```python
# Generate the Xs
# All words but the last one for each n-gram representation of the sentence
xs = input_sequences[:, :-1]

# The labels are the last word of the representation
# with the padding, so each word at a given moment
# will be part as the testing set
labels = input_sequences[:, -1]
```

#### One hot encoding for $y$
- Encode the labels with `keras`
	- #One-hot-encoding 
```python
ys = tf.keras.utisl.to_categorical(
	labels, num_classes = total_words
)
```
![[Captura de Pantalla 2021-09-21 a la(s) 21.18.05.png]]

## Train the model
#### A Forward LSTM
- Because there is not too much data we have to train the model for a lot of epochs => `epochs=500`
```python
model = Sequential()
model.add(Embedding(total_words, 64, 
					# Subtrac 1 because the last word is the target variable
				   input_length = max_sequence_len - 1
				   ))
model.add(LSTM(20))
model.add(Dense(total_words, activation = 'softmax'))
model.compile(loss='categorical_crossentropy',
			   optimizer = 'adam',
			   metrics = ['accuracy'])

history = model.fit(xs, ys, 
					epochs = 500, 
					verbose = 1)
```

## Predicting words
- Generate a new text from a seed text
	- SEED: `Laurence went to dubling`
```python
seed_text = "Joel went to Texas"
# predict the next words
next_words = 100

for _ in range(next_words):
	token_list = tokenizer.texts_to_sequences([seed_text])[0]
	token_list = pad_sequences(
		[token_list], maxlen = max_sequence_len - 1,
		padding = 'pre'
	)
	predicted = model.predict_classes(token_list, verbose = 0)
	output_word = ''
	for word, index in tokenizer.word_index.items()
		if index == predicted:
			output_word = word
			break
	seed_text += " " + output_word
print(seed_word)
```

### Second model
#### A `Bidirectional` LSTM
```python
model = Sequential()
model.add(Embedding(total_words, 64, 
					# Subtrac 1 because the last word is the target variable
				   input_length = max_sequence_len - 1
				   ))
model.add(Bidirectional(LSTM(20)))
model.add(Dense(total_words, activation = 'softmax'))
model.compile(loss='categorical_crossentropy',
			   optimizer = 'adam',
			   metrics = ['accuracy'])

history = model.fit(xs, ys, 
					epochs = 500, 
					verbose = 1)
```