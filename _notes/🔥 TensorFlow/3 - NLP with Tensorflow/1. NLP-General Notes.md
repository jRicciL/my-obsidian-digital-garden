---
---

# Natural Language Processing

### Content


## Word based encodings
- Training a neural network with just the letters could be a daunting task
- We need a really bing training set to create a meaningful vocabulary

## Using APIs

#### Keras Tokenizer
- Keras `tokenizer` => 
	- Removes puntuation
	- Converts to lowercase
	- Returns a `dict`:
		- `{'word': index}`

```python
tf.keras.preprocessing.text.Tokenizer(    
	num_words=None,    
	filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',    
	lower=True, 
	split=' ', 
	char_level=False, oov_token=None,    
	document_count=0, 
	**kwargs
)
```

##### Example

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

# List of sentences (texts)
# <CORPUS>
sentences = [
	'I love my dog',
	'I love my cat',
	'You love my dog'
]

# Now, instantiate the tokenizer
tokenizer = Tokenizer(num_words = 100) 
#  `num_words` are the top 100 frequent words
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

# returns an dictionary
print(word_index)
```

```
{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}
```

## Text to sequence
- `tokenizer.text_to_sequences`:
	- When testing or inferring, the new sample has to be coded with the same ==tokenizer==
	
```python
sequences = tokenizer.texts_to_sequences(sentences)
# Returns a list of lists with the words replaced
# by integers
```

#### Use special values for unseen words
```python
tokenizer = Tokenizer(
				num_words = 100, 
			    # something unique and distinct
			    oov_token = '<OOV>') 
```

## Padding
- We need to have uniformity on the inputs sizes
	- Therefore, the sentences should have the same size
	- For this purpose we will use [`pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences):

```python
tf.keras.preprocessing.sequence.pad_sequences(
	sequences, 
	maxlen=None, 
	dtype='int32', 
	padding='pre',        
	truncating='pre', 
	value=0.0
)
```

- If `maxlen` is not specified then the padding size will be obtained from the largest sentence.

##### Example

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
# FOR PADDING
from tensorflow.keras.preprocessing.sequence import pad_sequences

# List of sentences (texts)
# <CORPUS>
sentences = [
	'I love my dog',
	'I love my cat',
	'You love my dog',
	'Do you think my dog is amazing?'
]

# Now, instantiate the tokenizer
tokenizer = Tokenizer(num_words = 100,
					  oov_token = '<OOV>') 
sequences = tokenizer.texts_to_sequences(sentences)

# Padding
padded = pad_sequences(sequences)
print(padded)
```

## Sarcasm detection
-> Kaggle link

<div class="rich-link-card-container"><a class="rich-link-card" href="https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection" target="_blank">
	<div class="rich-link-image-container">
		<div class="rich-link-image" style="background-image: url('https://storage.googleapis.com/kaggle-datasets-images/30764/39238/f6d4a6b815ff246e22b348a81f39a324/dataset-card.jpg?t=2018-06-09-22-48-02')">
	</div>
	</div>
	<div class="rich-link-card-text">
		<h1 class="rich-link-card-title">News Headlines Dataset For Sarcasm Detection</h1>
		<p class="rich-link-card-description">
		High quality dataset for the task of Sarcasm Detection
		</p>
		<p class="rich-link-href">
		https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection
		</p>
	</div>
</a></div>

##### The dataset
![[Captura de Pantalla 2021-09-01 a la(s) 12.25.18.png]]

1. Load the dataset assuming it is in a `json` format:

```python
import json

with open('sarcasm.json', 'r') as f:
	datastore = json.load(f)
	
# Load each value to a respective list
sentences = []
labels = []
urls = []
for item in datastore:
	sentences.append(item['headline'])
	labels.append(item['is_sarcastic'])
	urls.append(item['article_link'])
```

2. Working with the ==Tokenizer==

```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenizer
tokenizer = Tokenizer(oov_token = '<OOV>')
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index

# Sequences
sequences = tokenizer.text_to_sequences(sentences)
# Padding => `post`
padded = pad_sequences(sequences, padding = 'post')
```

# Exercise
```python
sentences = []
labels = []
with open("/content/bbc-text.csv", 'r') as csvfile:
    ### START CODE HERE
    lines = csvfile.readlines()
    for line in lines[1:]:
      lab, s = line.split(',')
      for sw in stopwords:
        sw = f' {sw} '
        s = s.replace(sw, ' ').replace('  ', ' ')
      sentences.append(s)
      labels.append(lab)
```

```python
tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(sentences)
word_index = tokenizer.word_index
print(len(word_index))
```

```python
sequences = tokenizer.texts_to_sequences(sentences) 
padded = pad_sequences(sequences, padding = 'post')
print(padded[0])
print(padded.shape)
```

Tokenize labels
```python
label_tokenizer = Tokenizer()
label_tokenizer.fit_on_texts(labels)
label_word_index = label_tokenizer.word_index
label_seq = label_tokenizer.texts_to_sequences(labels)
```