---
---

# Embeddings

### Tensorflow IMDB
- Load the IMDB dataset ==>
```python
!pip install -q tensorflow-dataset

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np

# Load the IMDB dataset
imdb, info = tfds.load(
	"imdb_reviews", 
	with_info = True,
	as_supervised = True
)

# Get the training and test sets
train_data, test_data = imdb['train'], imdb['test']
```

- Now get the sentences and labels

```python
training_sentences = []
training_labels = []

testing_sentences = []
testing_labels = [] 

# Save the texts and labels to lists
for s, l in train_data:
	training_sentences.append(str(s.numpy()))
	training_labels.append(str(l.numpy()))
	
for s, l in train_data:
	testing_sentences.append(str(s.numpy()))
	testing_labels.append(str(l.numpy()))
	
# Turn the lists to numpy arrays
training_labels = np.array(training_labels)
testing_labels = np.array(testing_labels)
```

### Tokenize the sentences
- First define some values for the `tokenizer` and the embedding
	- Hyperparameters:
```python
vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = 'post'
oov_tok = '<OOV>'

```

##### Training
- Perform the *tokenization* and *padding*
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words = vocab_size,
					 oov_token = oov_tok)

# Use the training set to create the vocabulary
# Fit and transform the training texts
tokenizer.fit_on_texts(training_sentences)
word_index = tokenizer.word_index
# Convert to sequences
sequences = tokenizer.texts_to_sequences(training_sentences)
# Add the padding
padded = pad_sequences(sequences,
					  maxlen = max_length,
					  truncating = trunc_type)
```

##### Testing
- `testing` sentences are transformed with the fitted `tokenizer`
	- Becuase they are tookenized based on the `word_index` learned from the training test
```python
# Transform the testing sentences
testing_sequences = tokenizer.texts_to_sequences(testing_sentences)
testing_padded = pad_sequences(testing_sequences,
							  maxlen = max_length)
```

### The Neural Network with *Embedding*
Define the Neural Network using an [Embedding layer](https://keras.io/api/layers/core_layers/embedding/#embedding-layer)

#### Using a `Flatten`
- Using a `Flatten` layer after the embedding

```python
model = tf.keras.Sequential([
	tf.keras.layers.Embedding(input_dim    = vocab_size, 
							  output_dim   = emedding_dim, 
							  input_length = max_length),
	tf.keras.layers.Flatten(),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

```python
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
flatten (Flatten)            (None, 1920)              0         
_________________________________________________________________
dense (Dense)                (None, 6)                 11526     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,533
Trainable params: 171,533
```

![[Captura de Pantalla 2021-09-03 a la(s) 5.49.21.png]]

#### Using `GlobalAveragePooling2D`
- Using a `GlobalAveragePooling1D` layer after the embedding:
	- Less parameters -> Faster
	- But less accurate
```python
model = tf.keras.Sequential([
	tf.keras.layers.Ebedding(
		input_dim = vocab_size,
		output_dim = emedding_dim,
		input_length = max_length
	),
	tf.keras.layers.GlobalAveragePooling1D(),
	tf.keras.layers.Dense(6, activation = 'relu'),
	tf.keras.layers.Dense(1, activation = 'sigmoid')
])
```

```python
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_2 (Embedding)      (None, 120, 16)           160000    
_________________________________________________________________
global_average_pooling1d_1 ( (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 6)                 102       
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 7         
=================================================================
Total params: 160,109
Trainable params: 160,109
```
![[Captura de Pantalla 2021-09-03 a la(s) 5.57.19.png]]

### Training
Compile the model
```python
model.compile(loss = 'binary_crossentropy',
			  optimizer = 'adam',
			  metrics = ['accuracy'])
```

Train the model using the `padded` data and the `testing_padded` as the validation set.
```python
num_epochs = 10
model.fit(
	padded,
	training_labels_final,
	epochs = num_epochs,
	validation_data = (testing_padded, 
					   testing_labels_final)
)
```

## Visualize the Embedding
#### Get the *Embeddings*
- Get the `Embedding` layer:
```python
# The first layer was the embedding
e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # Shape: {vocab_size, embedding_dim}
```
```
(10000, 16)
```

- ==Reverse== the `word_index` dictionary:
```python
reverse_word_index = dict(
	[(value, key) for (key, value) in word_index.items()]
)
```
![[Captura de Pantalla 2021-09-03 a la(s) 18.58.52.png]]

#### Save the *Vectorized* words
```python
import io

out_v = io.open('vecs.tsv', 'w', encoding = 'utf-8')
out_m = io.open('meta.tsv', 'w', encoding = 'utf-8')

for word_num in range(1, vocab_size):
	# Get the word
	word = reverse_word_index[word_num]
	# Get the vector representation of the word
	embeddings = weights[word_num]
	out_m.write(word + '\n')
	out_v.write('\t'.join(
		[str(x) for x in embeddings] 
	) + '\n')
out_v.close()
out_m.close()
```