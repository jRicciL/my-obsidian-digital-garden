# Adjusting the learning rate dynamically

#learning-rate 
#hyperparameter_tunning 

***
- Common definitions
	- We will use a `LambdaLayer`^[[[8. W4 - Keras Lambda Layers]]]
	
```python
train_set = windowed_dataset(
	windowed_dataset(x_train, 
					 window_size, 
					 batch_size = 128
					 shuffle_buffer = shuffle_buffer_size
					))

model = tf.keras.models.Sequential([
	tf.keras.layers.Lambda(
		lambda x: tf.expand_dims(x, axis = -1), input_shape = [None]	
	),
	tf.keras.layers.SimpleRNN(40, return_sequences = True),
	tf.keras.layers.SimpleRNN(40),
	tf.keras.layers.Dense(1),
	tf.keras.layers.Lmbda(lambda x: x * 100.0)
])

```

### Learning rate Scheduler

#### Define the model
- **Adjust** the `learning_rate` using a `Callback` => *Scheduler*: `LearningRateScheduler`

```python
lr_schedule = tf.keras.callbacks.LearningRateScheduler(
	lambda epoch: 1e-8 * 10 ** (epoch / 20)
)
```

- Finish the model definition
	- Define the `optimizer` and compile the model
	- We will use the ==Huber== ( #HuberLoss) [loss function](https://en.wikipedia.org/wiki/Huber_loss):
		- Loss function that is less sensitive to outliers

```python
optimizer = tf.keras.optimizers.SDG(lr = 1e-8, momentum = 0.9)

model.compile(
	loss = tf.keras.losses.Huber(),
	optimizer = optimizer,
	metrics = ['mae']
)
```

#### Training to evaluate the best `learning_rate`
- Train the model using the ==callback==: `LearningRateScheduler`

```python
history = model.fit(
	train_set, epochs = 100,
	# Use the callback
	callbacks = [lr_schedule]
)
```

#### Find the best value of $a$
- Identify a candidate value to define the final `learning_rate`

![[Captura de Pantalla 2022-02-06 a la(s) 21.50.20.png]]

- Use the found value to recompile the model

```python
optimizer = tf.keras.optimizers.SDG(
	lr = 5e-5, # Value chosen from the image above
	momentum = 0.9
)
```


## Other notes
- [[Hyperparameter Tuning]]
- [[Hyperparameter_tunning_in_python]]
- [[Learning Rate]]