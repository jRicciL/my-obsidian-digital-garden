---
---

## Naive Bayes Classifiers
#NaiveBayes 

### A simple, probabilistic classifier family
- Based on simple probabilistic models:	
	- on how the data in *each class* might be generated
- ==Naive== => **Features are conditionally independet**
	- Because it assumes that each feature of an instance is independent (*little to no correlation*) of the other features
	- However, in practice, features tend to correlate to a some degree
- Are mathematically related to linear models
- Highly efficient learning and prediction
	- Because only simple statistics per class need to be estimated
- But generalization performance may worse than more sophisticated learning methods.
- Can be competitive for some tasks.
	- Specially for ==high dimensional datasets== or with sparse datasets

### Pros an cons
#### Pros:
1. Easy to understand
2. Simple, efficient parameter estimation
	- Fast during training and prediction
3. Works well with high dimensional data
4. Often useful as a baseline comparison against more sophisticated methods

#### Cons:
1. Assumption that features are conditionally independent given the class is not realistic.
2. Could have worse generalization performance if the assumption is not hold.
	- If there is some degree of correlation among features
3. Their confidence estimates for predictions are not very accurate.


***
## Naive Bayes Clf. Types

### Bernolli
<mark style='background-color: #9CE684 !important'>Binary features</mark>
- Used to classify text documents =>
	- According to the `presence/absence` of a given word

### Multinomial
<mark style='background-color: #9CE684 !important'>Discrete features</mark>

### Gaussian
<mark style='background-color: #9CE684 !important'>Continuous/real-valued features</mark>
- Statistics computed for each class/features =>
	- `mean` and `standard deviation`
- it assumes that the data for each class was generated by a simple class specific Gaussian distribution.
	- with a given $\mu$ and $\sigma$
- Predicting a class for a giving data point:
	- Estimating the probability that the Gaussian distribution of each class was most likely to have generate the data point

#### Decision boundary
- In general it is a parabolic curve within the classes (if the variance of each class is different)
	- It could be linear if the variance of the classes is the same

![[Captura de Pantalla 2021-11-23 a la(s) 12.55.01.png]]

#### Gaussian NBC in python

- `GaussianNB` supports the method `partial_fit()` during training.
	- Used to train the classifier incrementally

```python
from sklearn.naive_bayes import GaussianNB

X_train, X_test, y_train, y_test = train_test_split(X_C2, 
													y_C2, 
													random_state=0)

nbclf = GaussianNB().fit(X_train, y_train)
```